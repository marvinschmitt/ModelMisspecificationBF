{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "potential-tomorrow",
   "metadata": {},
   "source": [
    "# Project: Adversarial BayesFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-electron",
   "metadata": {},
   "source": [
    "# Experiment 3b: Diffusion Model (Unbounded Priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prime-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors\n",
    "\n",
    "import pickle\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../BayesFlow')))\n",
    "\n",
    "from bayesflow.trainers import ParameterEstimationTrainer\n",
    "from bayesflow.networks import InvertibleNetwork, InvariantNetwork\n",
    "from bayesflow.amortizers import SingleModelAmortizer\n",
    "from bayesflow.models import GenerativeModel\n",
    "from bayesflow.diagnostics import plot_sbc\n",
    "\n",
    "from abf_functions import *\n",
    "\n",
    "RERUN_GRID_EXPERIMENT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "varied-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\" : 20,\n",
    "    \"xtick.labelsize\" : 16,\n",
    "    \"ytick.labelsize\" : 16,\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"],\n",
    "})\n",
    "\n",
    "FILEFORMAT = 'pdf'\n",
    "DPI = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opposed-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(batch_size):\n",
    "    \"\"\"\n",
    "    Samples from the prior 'batch_size' times.\n",
    "    ----------\n",
    "\n",
    "    Arguments:\n",
    "    batch_size : int -- the number of samples to draw from the prior\n",
    "    ----------\n",
    "\n",
    "    Output:\n",
    "    theta : np.ndarray of shape (batch_size, theta_dim) -- the samples batch of parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Prior ranges for the simulator\n",
    "    \n",
    "    p_samples = np.random.gamma(5, 0.5, size=(batch_size, 5))\n",
    "    return p_samples.astype(np.float32)\n",
    "\n",
    "\n",
    "@njit\n",
    "def diffusion_trial(v, a, ndt, zr, dt, max_steps):\n",
    "    \"\"\"Simulates a trial from the diffusion model.\"\"\"\n",
    "\n",
    "    n_steps = 0.\n",
    "    x = a * zr\n",
    "\n",
    "    # Simulate a single DM path\n",
    "    while (x > 0 and x < a and n_steps < max_steps):\n",
    "\n",
    "        # DDM equation\n",
    "        x += v*dt + np.sqrt(dt) * np.random.normal()\n",
    "\n",
    "        # Increment step\n",
    "        n_steps += 1.0\n",
    "\n",
    "    rt = n_steps * dt\n",
    "    return rt + ndt if x > 0. else -rt - ndt\n",
    "\n",
    "@njit\n",
    "def diffusion_condition(n_trials, v, a, ndt, zr=0.5, dt=0.005, max_steps=1e4):\n",
    "    \"\"\"Simulates a diffusion process over an entire condition.\"\"\"\n",
    "\n",
    "    x = np.empty(n_trials)\n",
    "    for i in range(n_trials):\n",
    "        x[i] = diffusion_trial(v, a, ndt, zr, dt, max_steps)\n",
    "    return x\n",
    "\n",
    "\n",
    "@njit\n",
    "def diffusion_2_conds(params, n_trials, dt=0.005, max_steps=1e4):\n",
    "    \"\"\"\n",
    "    Simulates a diffusion process for 2 conditions with 5 parameters (v1, v2, a1, a2, ndt).\n",
    "    \"\"\"\n",
    "\n",
    "    n_trials_c1 = n_trials[0]\n",
    "    n_trials_c2 = n_trials[1]\n",
    "\n",
    "    v1, v2, a1, a2, ndt = params\n",
    "    rt_c1 = diffusion_condition(n_trials_c1, v1, a1, ndt,  dt=dt, max_steps=max_steps)\n",
    "    rt_c2 = diffusion_condition(n_trials_c2, v2, a2, ndt, dt=dt, max_steps=max_steps)\n",
    "    rts = np.concatenate((rt_c1, rt_c2))\n",
    "    return rts\n",
    "\n",
    "\n",
    "def batch_simulator(prior_samples, n_obs, dt=0.005, s=1.0, max_iter=1e4):\n",
    "    \"\"\"\n",
    "    Simulate multiple diffusion_model_datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    n_sim = prior_samples.shape[0]\n",
    "    sim_data = np.empty((n_sim, n_obs), dtype=np.float32)\n",
    "\n",
    "    n1 = n_obs//2\n",
    "    n2 = n_obs - n1\n",
    "\n",
    "    # Simulate diffusion data\n",
    "    for i in range(n_sim):\n",
    "        sim_data[i] = diffusion_2_conds(prior_samples[i], (n1, n2))\n",
    "\n",
    "    # Create condition labels\n",
    "    cond_arr = np.stack(n_sim * [np.concatenate((np.zeros(n1), np.ones(n2)))] )\n",
    "    sim_data = np.stack((sim_data, cond_arr), axis=-1)\n",
    "\n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "changed-mission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing networks from scratch.\n"
     ]
    }
   ],
   "source": [
    "D = 5  # 5 parameters in total\n",
    "\n",
    "summary_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    2,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 128},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 128},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 128},\n",
    "}\n",
    "\n",
    "class BottleneckSummaryNet(tf.keras.Model):\n",
    "    def __init__(self, inv_meta={}, n_out=10, activation_out=None):\n",
    "        super(BottleneckSummaryNet, self).__init__()\n",
    "\n",
    "        self.invariant_net = InvariantNetwork(inv_meta)\n",
    "        self.out_layer = tf.keras.layers.Dense(n_out, activation=activation_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out_inv = self.invariant_net(x)\n",
    "        out = self.out_layer(out_inv)\n",
    "        return out\n",
    "\n",
    "\n",
    "summary_net = BottleneckSummaryNet(inv_meta=summary_meta, \n",
    "                                   n_out=D*2,  # twice the required\n",
    "                                   activation_out=None  # linear\n",
    "                                  )\n",
    "\n",
    "\n",
    "inference_meta = {\n",
    "    'n_coupling_layers': 4,\n",
    "    's_args': {\n",
    "        'units': [128, 128],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [128, 128],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'n_params': 5,\n",
    "    'alpha': 1.9,\n",
    "    'permute': True\n",
    "}\n",
    "\n",
    "\n",
    "inference_net = InvertibleNetwork(inference_meta)\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)\n",
    "\n",
    "generative_model = GenerativeModel(prior, batch_simulator)\n",
    "\n",
    "trainer = ParameterEstimationTrainer(amortizer,\n",
    "                      generative_model,\n",
    "                      loss=mmd_kl_loss,\n",
    "                      learning_rate=0.0007,\n",
    "                      checkpoint_path='export_ckpt/mmd/ddm_unbounded',\n",
    "                      max_to_keep = 2\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-album",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9124428cc2234ef1b57a4e9e153d05e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = trainer.train_online(epochs=75, iterations_per_epoch=1000, batch_size=64, n_obs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-findings",
   "metadata": {},
   "source": [
    "## Validate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate (quick and dirty)\n",
    "p, x = trainer._forward_inference(300, 100)\n",
    "param_samples = amortizer.sample(x, n_samples=2000)\n",
    "param_means = param_samples.mean(axis=1)\n",
    "true_vs_estimated(p, param_means, ['v1', 'v2', 'a1', 'a2', 't0'], figsize=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sbc, x_sbc = trainer._forward_inference(5000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_samples_sbc = tf.concat([amortizer.sample(x_, n_samples=250) \n",
    "                               for x_ in tf.split(x_sbc, 20, axis=0)], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_sbc(param_samples_sbc, p_sbc, ['v1', 'v2', 'a1', 'a2', 't0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-assurance",
   "metadata": {
    "code_folding": [
     6,
     13,
     36,
     44
    ]
   },
   "outputs": [],
   "source": [
    "def contamination_dist_fast_guesses(percentiles, n):    \n",
    "    return np.random.uniform(0.1, percentiles[10], n)\n",
    "\n",
    "def contamination_dist_slow_responses(percentiles, n):   \n",
    "    return np.random.uniform(percentiles[75], 10.0, n)\n",
    "\n",
    "def contamination_dist_fast_and_slow(percentiles, n):\n",
    "    n1 = int(n/2)\n",
    "    n2 = n-n1\n",
    "    fast = contamination_dist_fast_guesses(percentiles, n1)\n",
    "    slow = contamination_dist_slow_responses(percentiles, n2)\n",
    "    return np.concatenate((fast, slow))\n",
    "\n",
    "def contaminate(x, contamination_dist, c=0.1):\n",
    "    \"\"\"\n",
    "    Contaminate the random variate vector x with contaminants according to fraction c \\in [0, 1]\n",
    "    \"\"\"\n",
    "    #x_contamination = x.copy()\n",
    "    \n",
    "    if not x.size > 0:\n",
    "        return x\n",
    "\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    n_contamination = int(n * c)\n",
    "    contamination_idx = np.random.default_rng().choice(n, size=n_contamination, replace=False)\n",
    "    \n",
    "    percentiles = np.percentile(x, range(101))\n",
    "    \n",
    "    sampled_contamination = contamination_dist(percentiles, n_contamination)\n",
    "    assert sampled_contamination.shape[0] == n_contamination\n",
    "    \n",
    "    \n",
    "    x[contamination_idx] = sampled_contamination\n",
    "    return x\n",
    "\n",
    "def split_posneg_contaminate(x, contamination_dist, c):\n",
    "    pos_idx = np.where(x>=0)\n",
    "    neg_idx = np.where(x<0)\n",
    "    \n",
    "    x[pos_idx] = contaminate(x[pos_idx], contamination_dist=contamination_dist, c=c)\n",
    "    x[neg_idx] = contaminate(x[neg_idx], contamination_dist=contamination_dist, c=c)\n",
    "    return x\n",
    "\n",
    "def contaminate_dm_data(x, contamination_dist, c=0.1):\n",
    "    x_copy = x.copy()\n",
    "    n_sim, n_obs, data_dim = x_copy.shape\n",
    "    for bi in range(n_sim):\n",
    "        x_copy[bi, :(n_obs//2), 0] = split_posneg_contaminate(x_copy[bi, :(n_obs//2), 0], contamination_dist, c=c)\n",
    "        x_copy[bi, (n_obs//2):, 0] = split_posneg_contaminate(x_copy[bi, (n_obs//2):, 0], contamination_dist, c=c)\n",
    "    return x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b37f7-47a9-4f24-aedc-0037743031ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "theta, x = trainer._forward_inference(300, 100)\n",
    "\n",
    "param_samples = trainer.network.sample(x, n_samples=2000)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "\n",
    "true_vs_estimated(theta, param_means, [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$t_0$'], figsize=(20,4),\n",
    "                 filename=f'plots/abf_ddm_unbounded_true_est_baseline.{FILEFORMAT}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c8b6c-5d02-4f3b-bfff-f7187f75f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast guesses\n",
    "theta, x = trainer._forward_inference(300, 100)\n",
    "x_o = contaminate_dm_data(x, contamination_dist=contamination_dist_fast_guesses, c=0.1)\n",
    "\n",
    "param_samples = trainer.network.sample(x_o, n_samples=2000)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "\n",
    "true_vs_estimated(theta, param_means, [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$t_0$'], figsize=(20,4),\n",
    "                 filename=f'plots/abf_ddm_unbounded_true_est_fast.{FILEFORMAT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532d6da-1818-4fca-b30a-cc7160ee8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow responses\n",
    "theta, x = trainer._forward_inference(300, 100)\n",
    "x_o = contaminate_dm_data(x, contamination_dist=contamination_dist_slow_responses, c=0.1)\n",
    "\n",
    "param_samples = trainer.network.sample(x_o, n_samples=2000)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "\n",
    "true_vs_estimated(theta, param_means, [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$t_0$'], figsize=(20,4),\n",
    "                  filename=f'plots/abf_ddm_unbounded_true_est_slow.{FILEFORMAT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1a84d-cc0c-4477-a815-186b7d96a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast guesses and slow responses\n",
    "theta, x = trainer._forward_inference(300, 100)\n",
    "x_o = contaminate_dm_data(x, contamination_dist=contamination_dist_fast_and_slow, c=0.1)\n",
    "\n",
    "param_samples = trainer.network.sample(x_o, n_samples=2000)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "\n",
    "true_vs_estimated(theta, param_means, [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$t_0$'], figsize=(20,4),\n",
    "                 filename=f'plots/abf_ddm_unbounded_true_est_fast_slow.{FILEFORMAT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae71f6f-f7c4-4a26-9f0b-74c53c9becbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"viridis\", n_colors=4)\n",
    "\n",
    "color_codes = {\n",
    "    \"No MMS\" : colors[0],\n",
    "    \"fast\" : colors[1],\n",
    "    \"slow\" : colors[2],\n",
    "    \"fast+slow\" : colors[3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = 10000\n",
    "\n",
    "\n",
    "# Pairplot of summary network outputs\n",
    "theta, x_uncontaminated = trainer._forward_inference(n_sim, 100)\n",
    "\n",
    "x_o_fast = contaminate_dm_data(x_uncontaminated, contamination_dist=contamination_dist_fast_guesses, c=0.1)\n",
    "x_o_slow = contaminate_dm_data(x_uncontaminated, contamination_dist=contamination_dist_slow_responses, c=0.1)\n",
    "x_o_fast_slow = contaminate_dm_data(x_uncontaminated, contamination_dist=contamination_dist_fast_and_slow, c=0.1)\n",
    "\n",
    "X = [x_o_fast, x_o_slow, x_o_fast_slow, x_uncontaminated]\n",
    "TASK_NAMES = ['fast', 'slow', 'fast+slow', 'No MMS']\n",
    "\n",
    "DF = (pd.DataFrame(np.array(trainer.network.summary_net(x)), \n",
    "                     columns=[r'$s_{%i}$'%i for i in range(1, 11)]) for x in X)\n",
    "\n",
    "df = pd.concat(DF,\n",
    "              keys=TASK_NAMES,\n",
    "              names=['MMS', None]\n",
    "              ).reset_index(level=0).reset_index(drop=True)\n",
    "\n",
    "def plot_subset(x, y, hue, mask, **kws):\n",
    "    sns.kdeplot(x=x[mask], y=y[mask], hue=hue[mask], fill=True, levels=10, antialiased=True, **kws)\n",
    "\n",
    "g = sns.PairGrid(df, hue=\"MMS\", height=3, palette=[color_codes[task_name] for task_name in TASK_NAMES] )\n",
    "\n",
    "g.map_diag(sns.kdeplot)\n",
    "\n",
    "g.map_lower(plot_subset, mask= \n",
    "            (\n",
    "                (df[\"MMS\"] == 'fast') |\n",
    "                (df[\"MMS\"] == 'slow') |\n",
    "                (df[\"MMS\"] == 'No MMS')\n",
    "           )\n",
    "           )\n",
    "\n",
    "\n",
    "g.map_upper(plot_subset, mask= \n",
    "            (\n",
    "                (df[\"MMS\"] == 'fast+slow') |\n",
    "                (df[\"MMS\"] == 'No MMS')\n",
    "           )\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "absmax = np.ceil(np.abs(df.iloc[:, 1:].values).max())\n",
    "ticks = np.arange(-100, 100, 5)\n",
    "ticks = ticks[(ticks>-absmax) & (ticks<absmax)]\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xlim(-absmax, absmax)\n",
    "    ax.set_ylim(-absmax, absmax)\n",
    "    \n",
    "    #ax.set_xticks(ticks)\n",
    "    #ax.set_yticks(ticks)\n",
    "    \n",
    "\n",
    "plt.savefig(f\"plots/abf_ddm_unbounded_pairplot.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(0.1,0.1))\n",
    "plt.gca().set_axis_off()\n",
    "handles = [mpatches.Patch(facecolor=color_codes[\"No MMS\"], label=r\"No MMS\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"fast\"], label=r\"$10\\%$ fast contamination\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"slow\"], label=r\"$10\\%$ slow contamination\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"fast+slow\"], label=r\"$5\\%$ fast and $5\\%$ slow contamination\")\n",
    "           ]\n",
    "plt.legend(handles=handles, loc=\"center\", ncol=4, title=\"\" ,fontsize=20, labelspacing=2)\n",
    "plt.savefig(f\"plots/abf_ddm_unbounded_pairplot_legend.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-nancy",
   "metadata": {},
   "source": [
    "# MMS paper: contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-alert",
   "metadata": {
    "code_folding": [
     6,
     11,
     35,
     46
    ]
   },
   "outputs": [],
   "source": [
    "def contamination_dist_fast_guesses(percentiles, n):    \n",
    "    return np.random.uniform(0.1, percentiles[10], n)\n",
    "\n",
    "def contamination_dist_slow_responses(percentiles, n):\n",
    "    return np.random.uniform(percentiles[75], 10.0, n)\n",
    "\n",
    "def contamination_dist_fast_and_slow(percentiles, n1, n2):\n",
    "    fast = contamination_dist_fast_guesses(percentiles, n1)\n",
    "    slow = contamination_dist_slow_responses(percentiles, n2)\n",
    "    return np.concatenate((fast, slow))\n",
    "\n",
    "def contaminate_grid(x, c_slow, c_fast):\n",
    "    \"\"\"\n",
    "    Contaminate the random variate vector x with contaminants according to fraction c \\in [0, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    if not x.size > 0:\n",
    "        return x\n",
    "\n",
    "    n = x.shape[0]\n",
    "    c = c_slow + c_fast\n",
    "    n_contamination = int(n * c)\n",
    "    contamination_idx = np.random.default_rng().choice(n, size=n_contamination, replace=False)\n",
    "    \n",
    "    percentiles = np.percentile(x, range(101))\n",
    "    \n",
    "    n_slow = int(n * c_slow)\n",
    "    n_fast = n_contamination - n_slow\n",
    "    sampled_contamination = contamination_dist_fast_and_slow(percentiles, n_slow, n_fast)\n",
    "    assert sampled_contamination.shape[0] == n_contamination\n",
    "    \n",
    "    \n",
    "    x[contamination_idx] = sampled_contamination\n",
    "    return x\n",
    "\n",
    "def split_posneg_contaminate(x, c_slow, c_fast):\n",
    "    pos_idx = np.where(x>=0)\n",
    "    neg_idx = np.where(x<0)\n",
    "    \n",
    "    x[pos_idx] = contaminate_grid(x[pos_idx], c_slow, c_fast)\n",
    "    x[neg_idx] = contaminate_grid(x[neg_idx], c_slow, c_fast)\n",
    "\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def contaminate_dm_data_grid(x, c_slow, c_fast):\n",
    "    x_copy = x.copy()\n",
    "    n_sim, n_obs, data_dim = x_copy.shape\n",
    "    for bi in range(n_sim):\n",
    "        x_copy[bi, :(n_obs//2), 0] = split_posneg_contaminate(x_copy[bi, :(n_obs//2), 0], c_slow, c_fast)\n",
    "        x_copy[bi, (n_obs//2):, 0] = split_posneg_contaminate(x_copy[bi, (n_obs//2):, 0], c_slow, c_fast)\n",
    "    return x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_slow = 101\n",
    "n_fast = 101\n",
    "slow_list = np.linspace(0.0, 0.14, num = n_slow)\n",
    "fast_list = np.linspace(0.0, 0.5, num = n_fast)\n",
    "\n",
    "slow_grid, fast_grid = np.meshgrid(slow_list, fast_list)\n",
    "\n",
    "if RERUN_GRID_EXPERIMENT:\n",
    "    _, x = trainer._forward_inference(200, 100)\n",
    "    z_psi = np.array(trainer.network.summary_net(x))\n",
    "\n",
    "    MMD = np.zeros((n_fast, n_slow))\n",
    "    for i in tqdm(range(n_fast)):\n",
    "        for j in range(n_slow):\n",
    "            c_slow = slow_grid[i, j]\n",
    "            c_fast = fast_grid[i, j]\n",
    "            _, x_o = generative_model(200, 100)\n",
    "\n",
    "            x_o = contaminate_dm_data_grid(x, c_slow=c_slow, c_fast=c_fast)\n",
    "            s_o = np.array(trainer.network.summary_net(x_o))\n",
    "\n",
    "            MMD[i, j] = float(maximum_mean_discrepancy(s_o, z_psi, squared=False))\n",
    "          \n",
    "    np.save(f\"data/MMD_grid_experiments/MMD_DDM_grid.npy\", MMD)\n",
    "\n",
    "MMD = np.load(f\"data/MMD_grid_experiments/MMD_DDM_grid.npy\")\n",
    "plt.figure()\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.pcolor(slow_grid, fast_grid, MMD, shading=\"nearest\", rasterized=True, vmin=0)\n",
    "plt.xlabel(r\"$\\lambda_{\\,\\mathrm{slow}}$\", fontsize=28)\n",
    "plt.ylabel(r\"$\\lambda_{\\,\\mathrm{fast}}$\", fontsize=28)\n",
    "plt.axhline(y=0.0, linestyle=\"--\", color=\"red\", alpha=.80, linewidth=3)\n",
    "plt.axvline(x=0.0, linestyle=\"--\", color=\"red\", alpha=.80, linewidth=3)\n",
    "\n",
    "plt.tick_params(labelsize=18)\n",
    "\n",
    "plt.xlim(-0.002, None)\n",
    "plt.ylim(-0.006, None)\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_xlabel(r\"MMD\", fontsize=22, labelpad=12)\n",
    "cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "plt.savefig(f\"plots/abf_ddm_unbounded_grid.{FILEFORMAT}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-princeton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-federation",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-possession",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddm_columnnames = [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$t_0$']\n",
    "\n",
    "# Simulate data and compute summary network output s\n",
    "theta, x = trainer._forward_inference(10000, 100)\n",
    "s = np.array(trainer.network.summary_net(x))\n",
    "\n",
    "n_pcs = 10\n",
    "pca = PCA(n_pcs)\n",
    "pcs = pca.fit_transform(s)\n",
    "\n",
    "df_theta = pd.DataFrame(theta, columns=ddm_columnnames)\n",
    "df_pcs = pd.DataFrame(pcs, columns=[f\"PC{j}\" for j in range(1, n_pcs+1)])\n",
    "df = pd.concat([df_theta, df_pcs], axis=1)\n",
    "r = df.corr().iloc[:5, 5:]\n",
    "\n",
    "r2 = pd.DataFrame(np.cumsum(pca.explained_variance_ratio_),\n",
    "                    index=[f\"PC{j}\" for j in range(1, n_pcs+1)],\n",
    "                    columns=[r\"$\\sum R^2$\"]).T\n",
    "\n",
    "#h2 = np.sum(np.array(r)**2, axis=1)\n",
    "#r[r'$h^2$']=(h2)\n",
    "\n",
    "r = r.append(r2)\n",
    "\n",
    "\n",
    "r_styled = r.style.background_gradient(cmap='coolwarm', vmin=-1, vmax=1, \n",
    "                           subset=(r.index[:-1], r.columns[:])).set_precision(2)\n",
    "r_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7914ae-397c-4bd6-b2f5-72bf49f99ae3",
   "metadata": {},
   "source": [
    "## BayesFlow vs. Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc04710-4d26-4546-934d-104dd52f3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset(x, y, hue, mask, **kws):\n",
    "    sns.kdeplot(x=x[mask], y=y[mask], hue=hue[mask], fill=True, levels=10, antialiased=True, **kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155ad0f-83bc-479c-8e5a-5cad3fb99b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"plasma\", 2)\n",
    "\n",
    "color_codes_approximators = {\n",
    "    'Stan' : colors[0],\n",
    "    'BayesFlow' : colors[1]\n",
    "}\n",
    "approximator_names = list(color_codes_approximators.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122dbd5-ad5b-4398-aaa9-c96b4a6a76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"export_stan_posteriors/data_posteriors.pkl\", 'rb') as f:\n",
    "    data_posteriors = pickle.load(f)\n",
    "with open(\"export_stan_posteriors/stan_posteriors.pkl\", 'rb') as f:\n",
    "    stan_posteriors = pickle.load(f)\n",
    "with open(\"export_stan_posteriors/params.pkl\", \"rb\") as f:\n",
    "    data_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-brick",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_posteriors(data_index):\n",
    "    \"\"\"\n",
    "    Utility to extract posteriors from stan and simultaneously sample from BayesFlow.\n",
    "    \"\"\"\n",
    "    \n",
    "    par_names_tex = [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$t_0$']\n",
    "    exp_keys = list(stan_posteriors.keys())\n",
    "    bayesflow_posteriors_dict = {}\n",
    "    stan_posteriors_dict = {}\n",
    "    for exp in exp_keys:\n",
    "        \n",
    "        # Convert samples from stan to df with nice columns\n",
    "        df_stan = pd.DataFrame(\n",
    "            stan_posteriors[exp][data_index],\n",
    "        )\n",
    "        df_stan = df_stan.drop('lp__', axis=1)\n",
    "        df_stan.columns = par_names_tex\n",
    "        \n",
    "        # Sample from bayesflow and convert to a data frame\n",
    "        df_bayesflow = pd.DataFrame(\n",
    "            amortizer.sample(\n",
    "                data_posteriors[exp][data_index][np.newaxis], len(df_stan)\n",
    "            ), \n",
    "            columns=par_names_tex\n",
    "        )\n",
    "        \n",
    "        stan_posteriors_dict[exp] = df_stan\n",
    "        bayesflow_posteriors_dict[exp] = df_bayesflow\n",
    "    return stan_posteriors_dict, bayesflow_posteriors_dict\n",
    "\n",
    "def plot_bivariate(df_stan, df_bayesflow, subsample_frac=None, save=False, save_suffix=None):\n",
    "    \"\"\"Helper to plot.\"\"\"\n",
    "    \n",
    "    # For speedy testing\n",
    "    if subsample_frac is not None:\n",
    "        df_stan = df_stan.sample(frac=subsample_frac, replace=False)\n",
    "        df_bayesflow = df_bayesflow.sample(frac=subsample_frac, replace=False)\n",
    "    \n",
    "    # Stack dataframes\n",
    "    df = pd.concat(\n",
    "        (df_stan, df_bayesflow),\n",
    "    ).reset_index(drop=True)\n",
    "    df['Method'] = ['Stan'] * len(df_stan) + ['BayesFlow'] * len(df_bayesflow)\n",
    "    \n",
    "    # Plot\n",
    "    g = sns.PairGrid(df, height=3, hue=\"Method\", \n",
    "                     palette=[color_codes_approximators[approximator_name] \n",
    "                              for approximator_name in approximator_names])\n",
    "    g.map_diag(sns.kdeplot, linewidth=3)\n",
    "    g.map_lower(sns.kdeplot, fill=True, levels=10, antialiased=True)\n",
    "    g.map_upper(sns.kdeplot, fill=True, levels=10, antialiased=True)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f\"plots/abf_ddm_unbounded_pairplot_stan_bf_{save_suffix}.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)\n",
    "    \n",
    "    plt.figure(figsize=(0.1,0.1))\n",
    "    plt.gca().set_axis_off()\n",
    "    handles = [mpatches.Patch(facecolor=color_codes_approximators[\"Stan\"], label=r\"Stan\"),\n",
    "               mpatches.Patch(facecolor=color_codes_approximators[\"BayesFlow\"], label=r\"BayesFlow\")\n",
    "               ]\n",
    "    plt.legend(handles=handles, loc=\"center\", ncol=4, title=\"\" ,fontsize=20, labelspacing=2)\n",
    "    if save:\n",
    "        plt.savefig(f\"plots/abf_ddm_unbounded_pairplot_stan_bf_legend_{save_suffix}.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_p = []\n",
    "bayesflow_p = []\n",
    "for i in range(len(data_posteriors['clean'])):\n",
    "    s, b = get_posteriors(i)\n",
    "    stan_p.append(s['clean'])\n",
    "    bayesflow_p.append(b['clean'])\n",
    "stan_p = np.array(stan_p)\n",
    "bayesflow_p = np.array(bayesflow_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be5868-63e3-4ef6-a7f9-e0f33380177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"clean\", \"fast\", \"slow\", \"fast_slow\"]\n",
    "\n",
    "MMDs = {\n",
    "\"clean\": [],\n",
    "\"fast\" : [],\n",
    "\"slow\" : [],\n",
    "\"fast_slow\": []\n",
    "}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Processing model: {model}\")\n",
    "    posteriors = data_posteriors[model]\n",
    "    for i in tqdm(range(len(posteriors))):\n",
    "        stan, bf = get_posteriors(i)\n",
    "        MMD = maximum_mean_discrepancy(np.array(stan[model], dtype=np.float32), np.array(bf[model], dtype=np.float32), squared=False)\n",
    "        MMDs[model].append(MMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba9b97-96d3-44b8-9ba3-4284c138223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_CI(x, ci_area=0.95):\n",
    "    q_lower = round((1.0 - ci_area) / 2, 5)\n",
    "    q_upper = round(1.0 - q_lower, 5)\n",
    "    return np.quantile(x, q_lower), np.quantile(x, q_upper)\n",
    "\n",
    "for model in models:\n",
    "    MMD = np.array(MMDs[model])\n",
    "    lower_bound, upper_bound = calculate_CI(MMD, ci_area=0.95)\n",
    "    print(f\"{model}: median={np.median(MMD):.2f}, [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    sns.kdeplot(MMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d11ef-bacf-40c3-851f-aea36d4fca6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9febfcd-ec85-49ba-bb3d-7d1543952d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f8be0-d58c-4eb2-800b-a57321108fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"clean\", \"fast\", \"slow\", \"fast_slow\"]\n",
    "\n",
    "MMDs = {\n",
    "\"clean\": None,\n",
    "\"fast\" : None,\n",
    "\"slow\" : None,\n",
    "\"fast_slow\": None\n",
    "}\n",
    "\n",
    "for model in models:\n",
    "    #print(f\"Processing model: {model}\")\n",
    "    \n",
    "    _, x_star = trainer._forward_inference(1000, 100)\n",
    "    s_star = trainer.network.summary_net(x_star)\n",
    "    \n",
    "    x_obs = data_posteriors[model]\n",
    "    s_obs = trainer.network.summary_net(x_obs)\n",
    "    \n",
    "    MMDs[model] = maximum_mean_discrepancy(s_star, s_obs, squared=False)\n",
    "    print(f\"Model {model}: 1000vs100, rMMD={MMDs[model]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d59506-4ecb-4350-a60a-c9fb043a4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD_bootstrap(x_star, x_test, N_BOOTSTRAP_ITERATIONS=10, n_samples_star=1000, n_samples_test=1000):\n",
    "    n_star = x_star.shape[0]\n",
    "    n_test = x_test.shape[0]\n",
    "    \n",
    "    MMD_bootstrap = np.empty(N_BOOTSTRAP_ITERATIONS)\n",
    "\n",
    "    for i in tqdm(range(N_BOOTSTRAP_ITERATIONS)):\n",
    "        idx_star = np.random.randint(0, n_star, size=n_samples_star)\n",
    "        idx_test = np.random.randint(0, n_test, size=n_samples_test)\n",
    "        \n",
    "        x_star_bootstrap = x_star[idx_star]\n",
    "        x_test_bootstrap = x_test[idx_test]\n",
    "        \n",
    "        s_star_bootstrap = np.array(trainer.network.summary_net(x_star_bootstrap))\n",
    "        s_test_bootstrap = np.array(trainer.network.summary_net(x_test_bootstrap))\n",
    "        \n",
    "        MMD_bootstrap[i] = float(maximum_mean_discrepancy(s_star_bootstrap, s_test_bootstrap, squared=False))\n",
    "        \n",
    "    return MMD_bootstrap\n",
    "\n",
    "def calculate_CI(x, ci_area=0.95):\n",
    "    q_lower = round((1.0 - ci_area) / 2, 5)\n",
    "    q_upper = round(1.0 - q_lower, 5)\n",
    "    return np.quantile(x, q_lower), np.quantile(x, q_upper)\n",
    "\n",
    "\n",
    "# Bootstrap 1000 vs. 100\n",
    "N_bootstrap_iterations = 100\n",
    "models = [\"clean\", \"fast\", \"slow\", \"fast_slow\"]\n",
    "\n",
    "MMDs = {\n",
    "\"clean\": np.empty(N_bootstrap_iterations),\n",
    "\"fast\" : np.empty(N_bootstrap_iterations),\n",
    "\"slow\" : np.empty(N_bootstrap_iterations),\n",
    "\"fast_slow\": np.empty(N_bootstrap_iterations)\n",
    "}\n",
    "\n",
    "for model in models:\n",
    "    #print(f\"Processing model: {model}\")\n",
    "    \n",
    "    _, x_star = trainer._forward_inference(1000, 100)\n",
    "    s_star = trainer.network.summary_net(x_star)\n",
    "    \n",
    "    x_obs = data_posteriors[model]\n",
    "    s_obs = trainer.network.summary_net(x_obs)\n",
    "    \n",
    "    MMDs[model] = MMD_bootstrap(x_star, x_obs, \n",
    "                                N_BOOTSTRAP_ITERATIONS=N_bootstrap_iterations,\n",
    "                                n_samples_star=1000, n_samples_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055167c2-bc08-4566-aa90-f8ec7282660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMDs_bootstrap = [MMDs['clean'], MMDs['fast'], MMDs['slow'], MMDs['fast_slow']]\n",
    "for i, MMD_b in zip(models, MMDs_bootstrap):\n",
    "    lower_bound, upper_bound = calculate_CI(MMD_b, ci_area=0.95)\n",
    "    median = np.median(MMD_b)\n",
    "    print(f\"M{i}: {median:.2f} [{lower_bound:.2f}, {upper_bound:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdbf84-ac00-4320-acc5-577685a75a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"clean\", \"fast\", \"slow\", \"fast_slow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_means = bayesflow_p.mean(1)\n",
    "stan_means = stan_p.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vs_estimated(data_params, bf_means, ['v1', 'v2', 'a1', 'a2', 't0'], figsize=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vs_estimated(data_params, stan_means, ['v1', 'v2', 'a1', 'a2', 't0'], figsize=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3196656-8d3d-4afc-bdde-102972f8f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [2, 2]\n",
    "models = ['clean', 'slow']\n",
    "\n",
    "for index, model in zip(indices, models):\n",
    "    test_stan, test_bf = get_posteriors(index)\n",
    "    posterior_MMD = maximum_mean_discrepancy(np.array(test_stan[model], dtype=np.float32), np.array(test_bf[model], dtype=np.float32), squared=False)\n",
    "    \n",
    "    x_obs = data_posteriors[model][index][np.newaxis]\n",
    "    s_obs = trainer.network.summary_net(x_obs)\n",
    "    _, x_star = trainer._forward_inference(1, 100)\n",
    "    z_x = trainer.network.summary_net(x_star)\n",
    "    summary_MMD = maximum_mean_discrepancy(z_x, s_obs, squared=False)\n",
    "    \n",
    "    print(f\"{model}, index {index}: Posterior MMD={posterior_MMD:.3f}, summary MMD={summary_MMD:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [2, 2]\n",
    "models = [\"clean\", \"slow\"]\n",
    "\n",
    "for index, model in zip(indices, models):\n",
    "    test_stan, test_bf = get_posteriors(index)\n",
    "    plot_bivariate(test_stan[model], test_bf[model], subsample_frac=None, save=True, save_suffix=model)\n",
    "    #plt.savefig(f\"plots/abf_ddm_pairplot_stan_bf_{model}.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f170c-9a51-4dc3-bff1-034bdaa318af",
   "metadata": {},
   "source": [
    "## Archived Code: Summary MMD tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef713460-22a5-4504-b289-c145c5b9b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_H0_samples = 1000\n",
    "N_star = 500\n",
    "\n",
    "_, x_star_fixed = trainer._forward_inference(N_star, 100)\n",
    "s_star_fixed = trainer.network.summary_net(x_star)\n",
    "\n",
    "MMD_H0 = np.empty(n_H0_samples)\n",
    "for i in tqdm(range(n_H0_samples)):\n",
    "    _, x_star_N1 = trainer._forward_inference(1, 100)\n",
    "    s_star_N1 = trainer.network.summary_net(x_star_N1)\n",
    "    MMD_H0[i] = maximum_mean_discrepancy(s_star_fixed, s_star_N1, squared=False)\n",
    "\n",
    "    x_obs_uncontaminated = data_posteriors[\"clean\"][2][np.newaxis]\n",
    "    s_obs_uncontaminated = trainer.network.summary_net(x_obs_uncontaminated)\n",
    "    MMD_uncontaminated = maximum_mean_discrepancy(s_star_fixed, s_obs_uncontaminated, squared=False)\n",
    "    \n",
    "    x_obs_contaminated = data_posteriors[\"slow\"][2][np.newaxis]\n",
    "    s_obs_contaminated = trainer.network.summary_net(x_obs_contaminated)\n",
    "    MMD_contaminated = maximum_mean_discrepancy(s_star_fixed, s_obs_contaminated, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58acc7-1ced-4ced-aede-ab5459b20f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vline_to_kde(x, kde_object, color, **kwargs):\n",
    "    kde_x, kde_y = kde_object.lines[0].get_data()\n",
    "    idx = np.argmin(np.abs(kde_x - x))\n",
    "    plt.plot([x, x], [0, kde_y[idx]], color=color, linewidth=4, **kwargs)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "alpha = 0.05\n",
    "H0_color = \"#287D8EFF\"\n",
    "uncontaminated_color = \"#55C667FF\"\n",
    "contaminated_color = \"#453781FF\"\n",
    "alpha_color = \"#481567FF\"\n",
    "\n",
    "kde = sns.kdeplot(MMD_H0, fill=False, linewidth=0, cut=0)\n",
    "sns.kdeplot(MMD_H0, fill=True, alpha=.12, color = H0_color, cut=0)\n",
    "\n",
    "draw_vline_to_kde(MMS_uncontaminated, kde, uncontaminated_color, label=r\"Uncontaminated\", linestyle=\"dashed\")\n",
    "draw_vline_to_kde(MMS_contaminated, kde, contaminated_color, label=r\"Slow contamination\", linestyle=\"dotted\")\n",
    "\n",
    "\n",
    "MMD_critical = np.quantile(MMD_H0, 1-alpha)\n",
    "draw_vline_to_kde(MMD_critical, kde, alpha_color, linestyle=\"dashed\", label=r\"Critical value\")\n",
    "\n",
    "sns.kdeplot(MMD_H0, fill=False, linewidth=3, color = H0_color, label=r\"$H_0$\", cut=0)\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12434f27-4068-4082-97e6-62e6c79d1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "MMD_H0 = np.empty(n_samples)\n",
    "MMD_contaminated = np.empty(n_samples)\n",
    "MMD_uncontaminated = np.empty(n_samples)\n",
    "\n",
    "for i in tqdm(range(n_samples)):\n",
    "    _, x_star_H0 = trainer._forward_inference(1, 100)\n",
    "    s_star_H0 = trainer.network.summary_net(x_star)\n",
    "    _, x_star = trainer._forward_inference(1, 100)\n",
    "    s_star = trainer.network.summary_net(x_star)\n",
    "    MMD_H0[i] = maximum_mean_discrepancy(s_star_H0, s_star, squared=False)\n",
    "\n",
    "for i in tqdm(range(n_samples)):\n",
    "    x_obs_uncontaminated = data_posteriors[\"clean\"][2][np.newaxis]\n",
    "    s_obs_uncontaminated = trainer.network.summary_net(x_obs_uncontaminated)\n",
    "    _, x_star = trainer._forward_inference(1, 100)\n",
    "    s_star = trainer.network.summary_net(x_star)\n",
    "    MMD_contaminated[i] = maximum_mean_discrepancy(s_star, s_obs_uncontaminated, squared=False)\n",
    "    \n",
    "for i in tqdm(range(n_samples)):\n",
    "    x_obs_contaminated = data_posteriors[\"slow\"][2][np.newaxis]\n",
    "    s_obs_contaminated = trainer.network.summary_net(x_obs_contaminated)\n",
    "    _, x_star = trainer._forward_inference(1, 100)\n",
    "    s_star = trainer.network.summary_net(x_star)\n",
    "    MMD_uncontaminated[i] = maximum_mean_discrepancy(s_star, s_obs_contaminated, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbbfef-dcfe-4c5d-ad12-e780a97c65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vline_to_kde(x, kde_object, color, **kwargs):\n",
    "    kde_x, kde_y = kde_object.lines[0].get_data()\n",
    "    idx = np.argmin(np.abs(kde_x - x))\n",
    "    plt.plot([x, x], [0, kde_y[idx]], color=color, linewidth=4, **kwargs)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "alpha = 0.05\n",
    "H0_color = \"#287D8EFF\"\n",
    "uncontaminated_color = \"#55C667FF\"\n",
    "contaminated_color = \"#453781FF\"\n",
    "alpha_color = \"#481567FF\"\n",
    "\n",
    "sns.kdeplot(MMD_H0, fill=False, alpha=.80, linewidth=3, color = H0_color, cut=0, label=r\"$H_0$\")\n",
    "sns.kdeplot(MMD_contaminated, fill=False, alpha=.80, linewidth=3, color = contaminated_color, cut=0, label=r\"contaminated\")\n",
    "sns.kdeplot(MMD_uncontaminated, fill=False, alpha=.80, linewidth=3, color = uncontaminated_color, cut=0, label=r\"uncontaminated\")\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e8420-4f25-41d6-9b3d-b8cfd0baa223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
