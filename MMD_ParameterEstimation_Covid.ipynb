{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebooks will replicate and build on the results obtained by estimating our custom Covid-19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../BayesFlow')))\n",
    "\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy.stats import binom, nbinom\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.networks import InvertibleNetwork, InvariantNetwork\n",
    "from bayesflow.amortizers import SingleModelAmortizer\n",
    "from bayesflow.trainers import ParameterEstimationTrainer\n",
    "from bayesflow.diagnostics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abf_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RERUN_BOOTSTRAP = True\n",
    "RERUN_POWER_ANALYSIS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\" : 20,\n",
    "    \"xtick.labelsize\" : 16,\n",
    "    \"ytick.labelsize\" : 16,\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"],\n",
    "})\n",
    "\n",
    "FILEFORMAT = 'pdf'\n",
    "DPI = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     23
    ]
   },
   "outputs": [],
   "source": [
    "confirmed_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "recovered_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "dead_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "\n",
    "\n",
    "confirmed_cases = pd.read_csv(confirmed_cases_url, sep=',')\n",
    "recovered_cases = pd.read_csv(recovered_cases_url, sep=',')\n",
    "dead_cases = pd.read_csv(dead_cases_url, sep=',')\n",
    "\n",
    "\n",
    "date_data_begin = datetime.date(2020,3,1)\n",
    "date_data_end = datetime.date(2020,5,21)\n",
    "\n",
    "\n",
    "format_date = lambda date_py: '{}/{}/{}'.format(date_py.month, date_py.day,\n",
    "                                                 str(date_py.year)[2:4])\n",
    "date_formatted_begin = format_date(date_data_begin)\n",
    "date_formatted_end = format_date(date_data_end)\n",
    "\n",
    "cases_obs =  np.array(\n",
    "    confirmed_cases.loc[confirmed_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "recovered_obs =  np.array(\n",
    "    recovered_cases.loc[recovered_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "\n",
    "dead_obs =  np.array(\n",
    "    dead_cases.loc[dead_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "\n",
    "data_germany = np.stack([cases_obs, recovered_obs, dead_obs]).T\n",
    "data_germany = np.diff(data_germany, axis=0)\n",
    "T_germany = data_germany.shape[0]\n",
    "N_germany = 83e6\n",
    "mean_g = np.mean(data_germany, axis=0)\n",
    "std_g = np.std(data_germany, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epidemiological Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     43,
     59
    ]
   },
   "outputs": [],
   "source": [
    "alpha_f = (0.7**2)*((1-0.7)/(0.17**2) - (1-0.7))\n",
    "beta_f = alpha_f*(1/0.7 - 1)\n",
    "\n",
    "\n",
    "def prior_sir():\n",
    "    \"\"\"\n",
    "    Implements batch sampling from a stationary prior over the parameters\n",
    "    of the non-stationary SIR model.\n",
    "    \"\"\"\n",
    "    \n",
    "    t1 = np.random.normal(loc=8, scale=3)\n",
    "    t2 = np.random.normal(loc=15, scale=1)\n",
    "    t3 = np.random.normal(loc=22, scale=1)\n",
    "    t4 = np.random.normal(loc=66, scale=1) \n",
    "    delta_t1 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    delta_t2 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    delta_t3 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    delta_t4 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    lambd0 = np.random.lognormal(mean=np.log(1.2), sigma=0.5)\n",
    "    lambd1 = np.random.lognormal(mean=np.log(0.6), sigma=0.5)\n",
    "    lambd2 = np.random.lognormal(mean=np.log(0.3), sigma=0.5)\n",
    "    lambd3 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
    "    lambd4 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
    "    mu = np.random.lognormal(mean=np.log(1/8), sigma=0.2)\n",
    "    f_i = np.random.beta(a=alpha_f, b=beta_f)\n",
    "    phi_i = stats.vonmises(kappa=0.01).rvs()\n",
    "    f_r = np.random.beta(a=alpha_f, b=beta_f)\n",
    "    phi_r = stats.vonmises(kappa=0.01).rvs()\n",
    "    f_d = np.random.beta(a=alpha_f, b=beta_f)\n",
    "    phi_d = stats.vonmises(kappa=0.01).rvs()\n",
    "    D_i = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
    "    D_r = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
    "    D_d = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
    "    E0 = np.random.gamma(shape=2, scale=30)\n",
    "    scale_I = np.random.gamma(shape=1, scale=5)\n",
    "    scale_R = np.random.gamma(shape=1, scale=5)\n",
    "    scale_D = np.random.gamma(shape=1, scale=5)\n",
    "    return [t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, \n",
    "            lambd0, lambd1, lambd2, lambd3, lambd4, mu, \n",
    "            f_i, phi_i, f_r, phi_r, f_d, phi_d, \n",
    "            D_i, D_r, D_d, E0, scale_I, scale_R, scale_D]\n",
    "\n",
    "\n",
    "def prior_secir():\n",
    "    \"\"\"\n",
    "    Implements batch sampling from a stationary prior over the parameters\n",
    "    of the non-stationary SIR model.\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = np.random.uniform(low=0.005, high=0.9)\n",
    "    beta = np.random.lognormal(mean=np.log(0.25), sigma=0.3)\n",
    "    gamma = np.random.lognormal(mean=np.log(1/6.5), sigma=0.5)\n",
    "    eta = np.random.lognormal(mean=np.log(1/3.2), sigma=0.3)\n",
    "    theta = np.random.uniform(low=1/14, high=1/3)\n",
    "    delta = np.random.uniform(low=0.01, high=0.3)\n",
    "    d = np.random.uniform(low=1/14, high=1/3)\n",
    "    return [alpha, beta, gamma, eta, theta, delta, d]\n",
    "\n",
    "\n",
    "def calc_lambda_array(sim_lag, lambd0, lambd1, lambd2, lambd3, lambd4, \n",
    "                      t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, T):\n",
    "    \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
    "    \n",
    "    # Array of initial lambdas\n",
    "    lambd0_arr = np.array([lambd0] * (t1+sim_lag))\n",
    "    \n",
    "    # Compute lambd1 array\n",
    "    if delta_t1 == 1:\n",
    "        lambd1_arr = np.array([lambd1] * (t2-t1))\n",
    "    else:\n",
    "        lambd1_arr = np.linspace(lambd0, lambd1, delta_t1)\n",
    "        lambd1_arr = np.append(lambd1_arr, [lambd1] * (t2-t1-delta_t1))\n",
    "        \n",
    "    # Compute lambd2 array\n",
    "    if delta_t2 == 1:\n",
    "        lambd2_arr = np.array([lambd2] * (t3-t2))\n",
    "    else:\n",
    "        lambd2_arr = np.linspace(lambd1, lambd2, delta_t2)\n",
    "        lambd2_arr = np.append(lambd2_arr, [lambd2] * (t3-t2-delta_t2))\n",
    "        \n",
    "    # Compute lambd3 array\n",
    "    if delta_t3 == 1:\n",
    "        lambd3_arr = np.array([lambd3] * (t4-t3))\n",
    "    else:\n",
    "        lambd3_arr = np.linspace(lambd3, lambd4, delta_t3)\n",
    "        lambd3_arr = np.append(lambd3_arr, [lambd3] * (t4-t3-delta_t3))\n",
    "        \n",
    "    # Compute lambd4 array\n",
    "    if delta_t4 == 1:\n",
    "        lambd4_arr = np.array([lambd4] * (T-t4))\n",
    "    else:\n",
    "        lambd4_arr = np.linspace(lambd3, lambd4, delta_t4)\n",
    "        lambd4_arr = np.append(lambd4_arr, [lambd4] * (T-t4-delta_t4))\n",
    "    \n",
    "    return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
    "\n",
    "    \n",
    "def non_stationary_SEICR(params_sir, params_secir, N, T, sim_diff=16, observation_model=True):\n",
    "    \"\"\"\n",
    "    Performs a forward simulation from the stationary SIR model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract parameters \n",
    "    t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, lambd0, lambd1, lambd2, lambd3, lambd4, mu, f_i, phi_i, f_r, phi_r, f_d, phi_d, delay_i, delay_r, delay_d, E0, scale_I, scale_R, scale_D = params_sir\n",
    "    alpha, beta, gamma, eta, theta, delta, d = params_secir\n",
    "    \n",
    "    # Round integer parameters\n",
    "    t1, t2, t3, t4 = int(round(t1)), int(round(t2)), int(round(t3)), int(round(t4))\n",
    "    delta_t1, delta_t2, delta_t3, delta_t4 = int(round(delta_t1)), int(round(delta_t2)), int(round(delta_t3)), int(round(delta_t4))\n",
    "    E0 = max(1, np.round(E0)) \n",
    "    delay_i = int(round(delay_i)) \n",
    "    delay_r = int(round(delay_r)) \n",
    "    delay_d = int(round(delay_d)) \n",
    "    \n",
    "    # Impose constraints\n",
    "    assert sim_diff > delay_i\n",
    "    assert sim_diff > delay_r\n",
    "    assert sim_diff > delay_d\n",
    "    assert t1 > 0 and t2 > 0 and t3 > 0 and t4 > 0\n",
    "    assert t1 < t2 < t3 < t4\n",
    "    assert delta_t1 > 0 and delta_t2 > 0 and delta_t3 > 0 and delta_t4 > 0\n",
    "    assert t2 - t1 >= delta_t1 and t3 - t2 >= delta_t2 and t4-t3 >= delta_t3 and T-t4 >= delta_t4\n",
    "\n",
    "    # Calculate lambda arrays\n",
    "    # Lambda0 is the initial contact rate which will be consecutively\n",
    "    # reduced via the government measures\n",
    "    sim_lag = sim_diff - 1\n",
    "    lambd_arr = calc_lambda_array(sim_lag, lambd0, lambd1, lambd2, lambd3, lambd4, \n",
    "                                  t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, T)\n",
    " \n",
    "    # Initial conditions\n",
    "    S, E, C, I, R, D = [N-E0], [E0], [0], [0], [0], [0]\n",
    "    \n",
    "    # Containers\n",
    "    I_news = []\n",
    "    R_news = []\n",
    "    D_news = []\n",
    "    \n",
    "    # Reported new cases\n",
    "    I_data = np.zeros(T)\n",
    "    R_data = np.zeros(T)\n",
    "    D_data = np.zeros(T)\n",
    "    fs_i = np.zeros(T)\n",
    "    fs_r = np.zeros(T)\n",
    "    fs_d = np.zeros(T)\n",
    " \n",
    "    # Simulate T-1 tiemsteps\n",
    "    for t in range(T+sim_lag):\n",
    "        \n",
    "        # Calculate new exposed cases\n",
    "        E_new = lambd_arr[t] * ((C[t] + beta*I[t])/N)*S[t]\n",
    "    \n",
    "        # Remove exposed from susceptible\n",
    "        S_t = S[t] - E_new\n",
    "        \n",
    "        # Calculate current exposed by adding new exposed and\n",
    "        # subtracting the exposed becoming carriers.\n",
    "        E_t = E[t] + E_new - gamma*E[t]\n",
    "        \n",
    "        # Calculate current carriers by adding the new exposed and subtracting\n",
    "        # those who will develop symptoms and become detected and those who\n",
    "        # will go through the disease asymptomatically.\n",
    "        C_t = C[t] + gamma*E[t] - (1-alpha)*eta*C[t] - alpha*theta*C[t]\n",
    "        \n",
    "        # Calculate current infected by adding the symptomatic carriers and \n",
    "        # subtracting the dead and recovered. The newly infected are just the \n",
    "        # carriers who get detected.\n",
    "        I_t = I[t] + (1-alpha)*eta*C[t] - (1-delta)*mu*I[t] - delta*d*I[t]\n",
    "        I_new = (1-alpha)*eta*C[t]\n",
    "        \n",
    "        # Calculate current recovered by adding the symptomatic and asymptomatic\n",
    "        # recovered. The newly recovered are only the detected recovered\n",
    "        R_t = R[t] + alpha*theta*C[t] + (1-delta)*mu*I[t]\n",
    "        R_new = (1-delta)*mu*I[t]\n",
    "        \n",
    "        # Calculate the current dead\n",
    "        D_t = D[t] + delta*d*I[t]\n",
    "        D_new = delta*d*I[t]\n",
    "        \n",
    "        # Ensure some numerical onstraints\n",
    "        S_t = np.clip(S_t, 0, N)\n",
    "        E_t = np.clip(E_t, 0, N)\n",
    "        C_t = np.clip(C_t, 0, N)\n",
    "        I_t = np.clip(I_t, 0, N)\n",
    "        R_t = np.clip(R_t, 0, N)\n",
    "        D_t = np.clip(D_t, 0, N)\n",
    "        \n",
    "        # Keep track of process over time\n",
    "        S.append(S_t)\n",
    "        E.append(E_t)\n",
    "        C.append(C_t)\n",
    "        I.append(I_t)\n",
    "        R.append(R_t)\n",
    "        D.append(D_t)\n",
    "        I_news.append(I_new)\n",
    "        R_news.append(R_new)\n",
    "        D_news.append(D_new)\n",
    "        \n",
    "        # From here, start adding new cases with delay D\n",
    "        # Note, we assume the same delay\n",
    "        if t >= sim_lag:\n",
    "            \n",
    "            # Compute lags and add to data arrays\n",
    "            fs_i[t-sim_lag] = (1-f_i)*(1 - np.abs( np.sin( (np.pi/7) * (t-sim_lag) - 0.5*phi_i)) )\n",
    "            fs_r[t-sim_lag] = (1-f_r)*(1 - np.abs( np.sin( (np.pi/7) * (t-sim_lag) - 0.5*phi_r)) )\n",
    "            fs_d[t-sim_lag] = (1-f_d)*(1 - np.abs( np.sin( (np.pi/7) * (t-sim_lag) - 0.5*phi_d)) )\n",
    "            I_data[t-sim_lag] = I_news[t-delay_i]\n",
    "            R_data[t-sim_lag] = R_news[t-delay_r]\n",
    "            D_data[t-sim_lag] = D_news[t-delay_d]\n",
    "            \n",
    "    # Compute weekly modulation\n",
    "    I_data = (1-fs_i) * I_data\n",
    "    R_data = (1-fs_r) * R_data\n",
    "    D_data = (1-fs_d) * D_data\n",
    "    \n",
    "    # Add noise\n",
    "    I_data = stats.t(df=4, loc=I_data, scale=np.sqrt(I_data)*scale_I).rvs()\n",
    "    R_data = stats.t(df=4, loc=R_data, scale=np.sqrt(R_data)*scale_R).rvs()\n",
    "    D_data = stats.t(df=4, loc=D_data, scale=np.sqrt(D_data)*scale_D).rvs()\n",
    "    \n",
    "    if observation_model:\n",
    "        return np.stack((I_data, R_data, D_data)).T\n",
    "    return np.stack((S, E, I, C, R, D)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def data_generator(n_sim, n_obs, N=None, sim_diff=21, N_min=10000, N_max=70000000):\n",
    "    \"\"\"\n",
    "    Runs the forward model 'batch_size' times by first sampling fromt the prior\n",
    "    theta ~ p(theta) and running x ~ p(x|theta).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Variable size N\n",
    "    if N is None:\n",
    "        N = np.random.randint(N_min, N_max)\n",
    "        \n",
    "    # Generate data\n",
    "    # x is a np.ndarray of shape (batch_size, n_obs, x_dim)\n",
    "    x = []\n",
    "    theta = []\n",
    "    for i in range(n_sim):\n",
    "        \n",
    "        # Reject meaningless simulaitons\n",
    "        x_i = None\n",
    "        while x_i is None:\n",
    "            try:\n",
    "                theta1 = prior_sir()\n",
    "                theta2 = prior_secir()\n",
    "                x_i = non_stationary_SEICR(theta1, theta2, N, n_obs, sim_diff=sim_diff)\n",
    "                x_i = (x_i - mean_g) / std_g\n",
    "            except:\n",
    "                 pass\n",
    "        # Simulate SECIR\n",
    "        x.append(x_i)\n",
    "        theta.append(theta1 + theta2)\n",
    "    x = np.array(x)\n",
    "    theta = np.array(theta)\n",
    "\n",
    "    # Convert to tensor, if specified \n",
    "    return theta.astype(np.float32), x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [r'$t_1$', r'$t_2$', r'$t_3$', r'$t_4$',\n",
    "               r'$\\Delta t_1$', r'$\\Delta t_2$', r'$\\Delta t_3$', r'$\\Delta t_4$',\n",
    "               r'$\\lambda_0$', r'$\\lambda_1$', r'$\\lambda_2$', r'$\\lambda_3$', r'$\\lambda_4$', \n",
    "               r'$\\mu$', r'$f_I$', r'$\\phi_I$',  r'$f_R$', r'$\\phi_R$',  \n",
    "               r'$f_D$', r'$\\phi_D$',\n",
    "               r'$L_I$', r'$L_R$', r'$L_D$', r'$E_0$', r'$\\sigma_I$', r'$\\sigma_R$', r'$\\sigma_D$', \n",
    "               r'$\\alpha$', r'$\\beta$', r'$\\gamma$',\n",
    "               r'$\\eta$', r'$\\theta$', r'$\\delta$', r'$d$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = partial(data_generator, N=N_germany, sim_diff=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "class MultiConvLayer(tf.keras.Model):\n",
    "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
    "    def __init__(self, n_filters=32, strides=1):\n",
    "        super(MultiConvLayer, self).__init__()\n",
    "        \n",
    "        self.convs = [\n",
    "            Conv1D(n_filters//2, kernel_size=f, strides=strides, \n",
    "                                   padding='causal', activation='relu', kernel_initializer='glorot_uniform')\n",
    "            for f in range(2, 8)\n",
    "        ]\n",
    "        self.dim_red = Conv1D(n_filters, 1, 1, activation='relu', kernel_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
    "        \n",
    "        out = tf.concat([conv(x) for conv in self.convs], axis=-1)\n",
    "        out = self.dim_red(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class MultiConvNet(tf.keras.Model):\n",
    "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
    "    def __init__(self, n_layers=3, n_filters=64, strides=1):\n",
    "        super(MultiConvNet, self).__init__()\n",
    "        \n",
    "        self.net = Sequential([\n",
    "            MultiConvLayer(n_filters, strides)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.lstm = LSTM(n_filters)\n",
    "        \n",
    "    def call(self, x, **args):\n",
    "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
    "        \n",
    "        out = self.net(x)\n",
    "        out = self.lstm(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class SummaryNet(tf.keras.Model):\n",
    "    def __init__(self, n_summary=192):\n",
    "        super(SummaryNet, self).__init__()\n",
    "        self.net_I = MultiConvNet(n_filters=n_summary//3)\n",
    "        self.net_R = MultiConvNet(n_filters=n_summary//3)\n",
    "        self.net_D = MultiConvNet(n_filters=n_summary//3)\n",
    "    \n",
    "    def call(self, x, **args):\n",
    "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
    "        \n",
    "        x = tf.split(x, 3, axis=-1)\n",
    "        x_i = self.net_I(x[0])\n",
    "        x_r = self.net_R(x[1])\n",
    "        x_d = self.net_D(x[2])\n",
    "        return tf.concat([x_i, x_r, x_d], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict={\n",
    "    'n_coupling_layers': 6,\n",
    "    's_args': {\n",
    "        'units': [192, 192, 192],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [192, 192, 192],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'alpha': 1.9,\n",
    "    'use_permutation': True,\n",
    "    'use_act_norm': True,\n",
    "    'n_params': len(param_names),\n",
    "}\n",
    "\n",
    "\n",
    "summary_net = SummaryNet()\n",
    "inference_net = InvertibleNetwork(meta_dict)\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_learning_rate = 0.0005\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    starter_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.99,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ParameterEstimationTrainer(\n",
    "    network=amortizer, \n",
    "    generative_model=data_gen,\n",
    "    loss=mmd_kl_loss,\n",
    "    learning_rate=learning_rate,\n",
    "    checkpoint_path=f'export_ckpt/mmd/covid19',\n",
    "    max_to_keep=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#losses = trainer.train_online(epochs=50, iterations_per_epoch=1000, batch_size=64, n_obs=T_germany)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x = trainer._forward_inference(10000, T_germany)\n",
    "s = np.array(trainer.network.summary_net(x))\n",
    "\n",
    "K = 192\n",
    "\n",
    "pca = PCA(K)\n",
    "pca.fit(s)\n",
    "\n",
    "cumsum_explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, K+1), cumsum_explained_variance_ratio, linewidth=4, color=\"dodgerblue\")\n",
    "plt.xlabel(\"Number of PCs\")\n",
    "plt.ylabel(r\"$\\sum R^2$\")\n",
    "plt.ylim(0)\n",
    "plt.xlim(0, 200)\n",
    "plt.plot([0, 192], [1, 1], linestyle=\"dashed\", color=\"grey\", linewidth=2)\n",
    "plt.plot([40, 40], [0, 0.95], linestyle=\"dashed\", color=\"darkgoldenrod\")\n",
    "\n",
    "plt.grid()\n",
    "sns.despine()\n",
    "plt.savefig(f\"plots/COVID_PCA_explained_variance.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_explained_variance_ratio[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Generate data from trainer's generative model\n",
    "_, x = trainer._forward_inference(1000, T_germany)\n",
    "s = np.array(trainer.network.summary_net(x))\n",
    "\n",
    "# Fit PCA to data from trainer\n",
    "pca = PCA(2)\n",
    "pca.fit(s)\n",
    "\n",
    "\n",
    "# Generate more data from trainer's generative model\n",
    "_, x_star = trainer._forward_inference(1000, T_germany)\n",
    "s_star = trainer.network.summary_net(x_star)\n",
    "\n",
    "# Project all candidate data onto the Principal Components\n",
    "s_star_proj = pca.transform(s_star)\n",
    "s_star_test_proj = pca.transform(s_star_test)\n",
    "s_1_proj = pca.transform(s_1)\n",
    "s_2_proj = pca.transform(s_2)\n",
    "s_3_proj = pca.transform(s_3)\n",
    "\n",
    "\n",
    "S = [s_star_proj, s_star_test_proj, s_1_proj, s_2_proj, s_3proj]\n",
    "TASK_NAMES = ['x_star', 'x_star_test', 'x_1', 'x_2', 'x_3']\n",
    "\n",
    "DF = (pd.DataFrame(s, \n",
    "                     columns=[r'$Proj_{%i}$'%i for i in range(1, 3)]) for s in S)\n",
    "\n",
    "df = pd.concat(DF,\n",
    "              keys=TASK_NAMES,\n",
    "              names=['Model', None]\n",
    "              ).reset_index(level=0)\n",
    "\n",
    "g = sns.PairGrid(df, hue=\"Model\", palette=['red', 'orange', 'green', 'blue', 'brown'], height=3)\n",
    "\n",
    "g.map_upper(plt.scatter, alpha=0.1)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.map_lower(sns.kdeplot, alpha=0.50)\n",
    "\n",
    "g.add_legend()\n",
    "plt.setp(g._legend.get_title(), fontsize=16)\n",
    "plt.setp(g._legend.get_texts(), fontsize=14)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Generate data from trainer's generative model\n",
    "_, x_star = trainer._forward_inference(1000, T_germany)\n",
    "s_star = trainer.network.summary_net(x_star)\n",
    "\n",
    "# Fit and project all data on separate PCAs based on them\n",
    "s_star_proj = PCA(2).fit_transform(s_star)\n",
    "s_star_test_proj = PCA(2).fit_transform(s_star_test)\n",
    "s_1_proj = PCA(2).fit_transform(s_1)\n",
    "s_2_proj = PCA(2).fit_transform(s_2)\n",
    "s_3_proj = PCA(2).fit_transform(s_3)\n",
    "\n",
    "\n",
    "S = [s_star_proj, s_star_test_proj, s_1_proj, s_2_proj, s_3_proj]\n",
    "TASK_NAMES = ['x_star', 'x_star_test', 'x_1', 'x_2', 'x_3']\n",
    "\n",
    "DF = (pd.DataFrame(s, \n",
    "                     columns=[r'$Proj_{%i}$'%i for i in range(1, 3)]) for s in S)\n",
    "\n",
    "df = pd.concat(DF,\n",
    "              keys=TASK_NAMES,\n",
    "              names=['Model', None]\n",
    "              ).reset_index(level=0)\n",
    "\n",
    "g = sns.PairGrid(df, hue=\"Model\", palette=['red', 'orange', 'green', 'blue', 'brown'], height=3)\n",
    "\n",
    "g.map_upper(plt.scatter, alpha=0.1)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.map_lower(sns.kdeplot, alpha=0.50)\n",
    "\n",
    "g.add_legend()\n",
    "plt.setp(g._legend.get_title(), fontsize=16)\n",
    "plt.setp(g._legend.get_texts(), fontsize=14)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/COVID_model_data/data_model_star_test.pkl', 'rb') as f:\n",
    "    x_star_test = pickle.load(f)['x']\n",
    "with open('data/COVID_model_data/data_model_1.pkl', 'rb') as f:\n",
    "    x_1 = pickle.load(f)['x']\n",
    "with open('data/COVID_model_data/data_model_2.pkl', 'rb') as f:\n",
    "    x_2 = pickle.load(f)['x']\n",
    "with open('data/COVID_model_data/data_model_3.pkl', 'rb') as f:\n",
    "    x_3 = pickle.load(f)['x']\n",
    "\n",
    "_, x_star = trainer._forward_inference(1000, T_germany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD_bootstrap(x_star, x_test, N_BOOTSTRAP_ITERATIONS=10, n_samples_star=1000, n_samples_test=1000):\n",
    "    n_star = x_star.shape[0]\n",
    "    n_test = x_test.shape[0]\n",
    "    \n",
    "    MMD_bootstrap = np.empty(N_BOOTSTRAP_ITERATIONS)\n",
    "\n",
    "    for i in tqdm(range(N_BOOTSTRAP_ITERATIONS)):\n",
    "        idx_star = np.random.randint(0, n_star, size=n_samples_star)\n",
    "        idx_test = np.random.randint(0, n_test, size=n_samples_test)\n",
    "        \n",
    "        x_star_bootstrap = x_star[idx_star]\n",
    "        x_test_bootstrap = x_test[idx_test]\n",
    "        \n",
    "        s_star_bootstrap = np.array(trainer.network.summary_net(x_star_bootstrap))\n",
    "        s_test_bootstrap = np.array(trainer.network.summary_net(x_test_bootstrap))\n",
    "        \n",
    "        MMD_bootstrap[i] = float(maximum_mean_discrepancy(s_star_bootstrap, s_test_bootstrap, squared=False))\n",
    "        \n",
    "    return MMD_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_CI(x, ci_area=0.95):\n",
    "    q_lower = round((1.0 - ci_area) / 2, 5)\n",
    "    q_upper = round(1.0 - q_lower, 5)\n",
    "    return np.quantile(x, q_lower), np.quantile(x, q_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_star = 1000\n",
    "N_BOOTSTRAP_ITERATIONS = 100\n",
    "\n",
    "for n_samples_test in [1,2,5]:\n",
    "    if RERUN_BOOTSTRAP:\n",
    "        print(f\"Started Bootstrap for n_samples_test={n_samples_test}\")\n",
    "        MMD_star_bootstrap = MMD_bootstrap(x_star, x_star_test, N_BOOTSTRAP_ITERATIONS=N_BOOTSTRAP_ITERATIONS, n_samples_star=n_samples_star, n_samples_test=n_samples_test)\n",
    "        MMD_1_bootstrap    = MMD_bootstrap(x_star, x_1,         N_BOOTSTRAP_ITERATIONS=N_BOOTSTRAP_ITERATIONS, n_samples_star=n_samples_star, n_samples_test=n_samples_test)\n",
    "        MMD_2_bootstrap    = MMD_bootstrap(x_star, x_2,         N_BOOTSTRAP_ITERATIONS=N_BOOTSTRAP_ITERATIONS, n_samples_star=n_samples_star, n_samples_test=n_samples_test)\n",
    "        MMD_3_bootstrap    = MMD_bootstrap(x_star, x_3,         N_BOOTSTRAP_ITERATIONS=N_BOOTSTRAP_ITERATIONS, n_samples_star=n_samples_star, n_samples_test=n_samples_test)\n",
    "\n",
    "        np.save(f\"data/MMD_bootstrapping/MMD_star_bootstrap_N{n_samples_test}.npy\", MMD_star_bootstrap)\n",
    "        np.save(f\"data/MMD_bootstrapping/MMD_1_bootstrap_N{n_samples_test}.npy\", MMD_1_bootstrap)\n",
    "        np.save(f\"data/MMD_bootstrapping/MMD_2_bootstrap_N{n_samples_test}.npy\", MMD_2_bootstrap)\n",
    "        np.save(f\"data/MMD_bootstrapping/MMD_3_bootstrap_N{n_samples_test}.npy\", MMD_3_bootstrap)\n",
    "    \n",
    "    MMD_star_bootstrap = np.load(f\"data/MMD_bootstrapping/MMD_star_bootstrap_N{n_samples_test}.npy\")\n",
    "    MMD_1_bootstrap = np.load(f\"data/MMD_bootstrapping/MMD_1_bootstrap_N{n_samples_test}.npy\")\n",
    "    MMD_2_bootstrap = np.load(f\"data/MMD_bootstrapping/MMD_2_bootstrap_N{n_samples_test}.npy\")\n",
    "    MMD_3_bootstrap = np.load(f\"data/MMD_bootstrapping/MMD_3_bootstrap_N{n_samples_test}.npy\")\n",
    "    MMDs_bootstrap = [MMD_star_bootstrap, MMD_1_bootstrap, MMD_2_bootstrap, MMD_3_bootstrap]\n",
    "\n",
    "    for i, MMD_b in zip([\"star\", \"1\", \"2\", \"3\"], MMDs_bootstrap):\n",
    "        lower_bound, upper_bound = calculate_CI(MMD_b, ci_area=0.95)\n",
    "        median = np.median(MMD_b)\n",
    "        print(f\"N = {n_samples_test}:   M{i}: {median:.3f} [{lower_bound:.3f}, {upper_bound:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MMDs = [MMD_star, MMD_1, MMD_2, MMD_3]\n",
    "MMD_bootstraps = [MMD_star_bootstrap, MMD_1_bootstrap, MMD_2_bootstrap, MMD_3_bootstrap]\n",
    "colors = [\"red\", \"firebrick\", \"orange\", \"blue\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(4):\n",
    "    sns.kdeplot(MMD_bootstraps[i], ax=ax, label=f\"M{i}\")\n",
    "    ax.legend()\n",
    "    ax.set_title(r\"$\\widehat{rMMD}$ for 100 bootstraps of $x^*$ and $x_{test}$\")\n",
    "    ax.set_xlabel(r\"$\\widehat{rMMD}$\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Analysis for $N=1,2,5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_star = data_gen\n",
    "from covid_models.covid19_ablation_intervention import data_gen as data_gen_1\n",
    "from covid_models.covid19_ablation_observation import data_gen as data_gen_2\n",
    "from covid_models.covid19_ablation_carrier import data_gen as data_gen_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD_power_analysis_generic(G_star, N_star, G_test, N_test, summary_network, number_power_simulations, number_H0_simulations=1000, n_obs=None, alpha=.05):\n",
    "    # Calculate MMD_H0: Simulate MMD(A|B) with A~G_star and B~G_star\n",
    "    MMD_H0 = np.empty(number_H0_simulations)\n",
    "    _, x_star = G_star(N_star, n_obs)\n",
    "    s_star = summary_network(x_star)\n",
    "    for i in tqdm(range(number_H0_simulations), desc=\"Compute MMD under H0\"):\n",
    "        _, x_star_prime = G_star(N_test, n_obs)\n",
    "        s_star_prime = summary_network(x_star_prime)\n",
    "        MMD_H0[i] = float(maximum_mean_discrepancy(s_star, s_star_prime, squared=False))\n",
    "    \n",
    "    \n",
    "    # Simulate data from the test model and compare its MMDs against H0 MMD disto\n",
    "    MMDs_test = np.empty(number_power_simulations)\n",
    "    _, x_star = G_star(N_star, n_obs)\n",
    "    s_star = summary_network(x_star)\n",
    "    for i in tqdm(range(number_power_simulations), desc=\"Simulate data from G_test and compute MMD\"):\n",
    "        _, x_test = G_test(N_test, n_obs)\n",
    "        s_test = summary_network(x_test)\n",
    "        \n",
    "        MMDs_test[i] = float(maximum_mean_discrepancy(s_star, s_test, squared=False))\n",
    "\n",
    "    MMD_critical = np.quantile(MMD_H0, 1-alpha)    \n",
    "    power = (MMDs_test > MMD_critical).mean()      \n",
    "\n",
    "    return {\"power\" : power,\n",
    "            \"MMD_H0\" : MMD_H0,\n",
    "            \"MMDs_test\" : MMDs_test,\n",
    "            \"MMD_critical\" : MMD_critical,\n",
    "            \"alpha\" : alpha,\n",
    "            \"N\" : N_test\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RERUN_POWER_ANALYSIS:\n",
    "    N_star = 1000\n",
    "    N_test = 1\n",
    "    number_power_simulations = 1000\n",
    "    number_H0_simulations = 1000\n",
    "\n",
    "    models = [1, 2, 3]\n",
    "    for model in models:\n",
    "        print(f\"Power analysis for model {model}\")\n",
    "        data_gen = globals()[f\"data_gen_{model}\"]\n",
    "        power_result = MMD_power_analysis_generic(\n",
    "            G_star = trainer._forward_inference,\n",
    "            N_star = N_star,\n",
    "            G_test = data_gen,\n",
    "            N_test = N_test,\n",
    "            summary_network = trainer.network.summary_net,\n",
    "            number_power_simulations = number_power_simulations,\n",
    "            number_H0_simulations = number_H0_simulations,\n",
    "            n_obs = T_germany\n",
    "            )\n",
    "\n",
    "        with open(f\"data/COVID_power/covid_power_N{N_test}_M{model}.pkl\", 'wb') as f:\n",
    "            pickle.dump(power_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RERUN_POWER_ANALYSIS:\n",
    "    N_star = 1000\n",
    "    N_test = 2\n",
    "    number_power_simulations = 1000\n",
    "    number_H0_simulations = 1000\n",
    "\n",
    "    models = [1, 2, 3]\n",
    "    for model in models:\n",
    "        print(f\"Power analysis for model {model}\")\n",
    "        data_gen = globals()[f\"data_gen_{model}\"]\n",
    "        power_result = MMD_power_analysis_generic(\n",
    "            G_star = trainer._forward_inference,\n",
    "            N_star = N_star,\n",
    "            G_test = data_gen,\n",
    "            N_test = N_test,\n",
    "            summary_network = trainer.network.summary_net,\n",
    "            number_power_simulations = number_power_simulations,\n",
    "            number_H0_simulations = number_H0_simulations,\n",
    "            n_obs = T_germany\n",
    "            )\n",
    "\n",
    "        with open(f\"data/COVID_power/covid_power_N{N_test}_M{model}.pkl\", 'wb') as f:\n",
    "            pickle.dump(power_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RERUN_POWER_ANALYSIS:\n",
    "    N_star = 1000\n",
    "    N_test = 5\n",
    "    number_power_simulations = 1000\n",
    "    number_H0_simulations = 1000\n",
    "\n",
    "    models = [1, 2, 3]\n",
    "    for model in models:\n",
    "        print(f\"Power analysis for model {model}\")\n",
    "        data_gen = globals()[f\"data_gen_{model}\"]\n",
    "        power_result = MMD_power_analysis_generic(\n",
    "            G_star = trainer._forward_inference,\n",
    "            N_star = N_star,\n",
    "            G_test = data_gen,\n",
    "            N_test = N_test,\n",
    "            summary_network = trainer.network.summary_net,\n",
    "            number_power_simulations = number_power_simulations,\n",
    "            number_H0_simulations = number_H0_simulations,\n",
    "            n_obs = T_germany\n",
    "            )\n",
    "\n",
    "        with open(f\"data/COVID_power/covid_power_N{N_test}_M{model}.pkl\", 'wb') as f:\n",
    "            pickle.dump(power_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_powers(N_test, models=[1,2,3]):\n",
    "    for model in models:\n",
    "        with open(f\"data/COVID_power/covid_power_N{N_test}_M{model}.pkl\", \"rb\") as f:\n",
    "            power_result = pickle.load(f)\n",
    "\n",
    "        MMD_H0 = power_result[\"MMD_H0\"]\n",
    "        MMDs_test = power_result[\"MMDs_test\"]\n",
    "        MMD_critical = power_result[\"MMD_critical\"]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        sns.histplot(MMD_H0, linewidth=3, label=r\"$H_0$\", color=\"royalblue\")\n",
    "        sns.histplot(MMDs_test, ax=ax, linewidth=3, label=r\"$\\mathcal{M}_{\\mathrm{test}}$\", color=\"seagreen\")\n",
    "        ax.set_title(r'Model $\\mathcal{M}_%d$ ; $1-\\beta=%.3f, N_{test}=%d$'%(model, power_result[\"power\"], N_test))\n",
    "        ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "        ax.axvline(x=MMD_critical, color=\"firebrick\", linewidth=2, linestyle=\"dashed\", label=r\"$\\alpha$\")\n",
    "        ax.legend()\n",
    "        plt.savefig(f\"plots/COVID_power_N{N_test}_M{model}.{FILEFORMAT}\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_powers(N_test=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_powers(N_test=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_powers(N_test=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_powers(N_test=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data w.r.t. $MMD|H_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "recovered_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "dead_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "\n",
    "\n",
    "confirmed_cases = pd.read_csv(confirmed_cases_url, sep=',')\n",
    "recovered_cases = pd.read_csv(recovered_cases_url, sep=',')\n",
    "dead_cases = pd.read_csv(dead_cases_url, sep=',')\n",
    "\n",
    "\n",
    "date_data_begin = datetime.date(2020,3,1)\n",
    "date_data_end = datetime.date(2020,5,21)\n",
    "\n",
    "\n",
    "format_date = lambda date_py: '{}/{}/{}'.format(date_py.month, date_py.day,\n",
    "                                                 str(date_py.year)[2:4])\n",
    "date_formatted_begin = format_date(date_data_begin)\n",
    "date_formatted_end = format_date(date_data_end)\n",
    "\n",
    "cases_obs =  np.array(\n",
    "    confirmed_cases.loc[confirmed_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "recovered_obs =  np.array(\n",
    "    recovered_cases.loc[recovered_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "\n",
    "dead_obs =  np.array(\n",
    "    dead_cases.loc[dead_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "\n",
    "data_germany = np.stack([cases_obs, recovered_obs, dead_obs]).T\n",
    "data_germany = np.diff(data_germany, axis=0)\n",
    "T_germany = data_germany.shape[0]\n",
    "N_germany = 83e6\n",
    "mean_g = np.mean(data_germany, axis=0)\n",
    "std_g = np.std(data_germany, axis=0)\n",
    "\n",
    "data_germany_tensor = data_germany[np.newaxis, ...]\n",
    "data_germany_tensor.shape\n",
    "data_germany_standardized = (data_germany_tensor - mean_g) / std_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = T_germany\n",
    "N_star = 1000\n",
    "N_test = 1\n",
    "number_H0_simulations = 10000\n",
    "G_star = data_gen_star\n",
    "summary_network = trainer.network.summary_net\n",
    "\n",
    "# Calculate MMD_H0: Simulate MMD(A|B) with A~G_star and B~G_star\n",
    "MMD_H0 = np.empty(number_H0_simulations)\n",
    "_, x_star = G_star(N_star, n_obs)\n",
    "s_star = summary_network(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(range(number_H0_simulations), desc=\"Compute MMD under H0\"):\n",
    "    _, x_star_prime = G_star(N_test, n_obs)\n",
    "    s_star_prime = summary_network(x_star_prime)\n",
    "    MMD_H0[i] = float(maximum_mean_discrepancy(s_star, s_star_prime, squared=False))\n",
    "    \n",
    "np.save(\"data/COVID_real_world_analysis/MMD_H0_N1.npy\", MMD_H0)\n",
    "np.save(\"data/COVID_real_world_analysis/MMD_H0_N1_x_star.npy\", x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.load(\"data/COVID_real_world_analysis/MMD_H0_N1_x_star.npy\")\n",
    "\n",
    "s_germany_standardized = np.array(trainer.network.summary_net(data_germany_standardized))\n",
    "s_germany_standardized.shape\n",
    "MMD_germany_standardized = float(maximum_mean_discrepancy(s_germany_standardized, s_star, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vline_to_kde(x, kde_object, color, **kwargs):\n",
    "    kde_x, kde_y = kde_object.lines[0].get_data()\n",
    "    idx = np.argmin(np.abs(kde_x - x))\n",
    "    plt.plot([x, x], [0, kde_y[idx]], color=color, linewidth=4, **kwargs)\n",
    "\n",
    "bw_factor = 1.5\n",
    "plt.figure(figsize=(8, 4))\n",
    "alpha = 0.05\n",
    "H0_color = \"#287D8EFF\"\n",
    "data_germany_color = \"#55C667FF\"\n",
    "alpha_color = \"#481567FF\"\n",
    "\n",
    "kde = sns.kdeplot(MMD_H0, fill=False, linewidth=0, bw_adjust=bw_factor)\n",
    "sns.kdeplot(MMD_H0, fill=True, alpha=.12, color = H0_color, bw_adjust=bw_factor)\n",
    "\n",
    "MMD_H0 = np.load(\"data/COVID_real_world_analysis/MMD_H0_N1.npy\")\n",
    "draw_vline_to_kde(MMD_germany_standardized, kde, data_germany_color, label=r\"Germany data\")\n",
    "\n",
    "MMD_critical = np.quantile(MMD_H0, 1-alpha)\n",
    "draw_vline_to_kde(MMD_critical, kde, alpha_color, linestyle=\"dashed\", label=r\"Critical value\")\n",
    "\n",
    "sns.kdeplot(MMD_H0, fill=False, linewidth=3, color = H0_color, label=r\"$H_0$\", bw_adjust=bw_factor)\n",
    "\n",
    "\n",
    "plt.xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "sns.despine()\n",
    "plt.savefig(f\"plots/COVID_real_data_MMD_H0.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor([1.,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
