{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebooks will replicate the results obtained by estimating our custom Covid-19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../BayesFlow')))\n",
    "\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import binom, nbinom\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.networks import InvertibleNetwork, InvariantNetwork\n",
    "from bayesflow.amortizers import SingleModelAmortizer\n",
    "from bayesflow.trainers import ParameterEstimationTrainer\n",
    "from bayesflow.diagnostics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abf_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     23
    ]
   },
   "outputs": [],
   "source": [
    "confirmed_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "recovered_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
    "dead_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "\n",
    "\n",
    "confirmed_cases = pd.read_csv(confirmed_cases_url, sep=',')\n",
    "recovered_cases = pd.read_csv(recovered_cases_url, sep=',')\n",
    "dead_cases = pd.read_csv(dead_cases_url, sep=',')\n",
    "\n",
    "\n",
    "date_data_begin = datetime.date(2020,3,1)\n",
    "date_data_end = datetime.date(2020,5,21)\n",
    "\n",
    "\n",
    "format_date = lambda date_py: '{}/{}/{}'.format(date_py.month, date_py.day,\n",
    "                                                 str(date_py.year)[2:4])\n",
    "date_formatted_begin = format_date(date_data_begin)\n",
    "date_formatted_end = format_date(date_data_end)\n",
    "\n",
    "cases_obs =  np.array(\n",
    "    confirmed_cases.loc[confirmed_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "recovered_obs =  np.array(\n",
    "    recovered_cases.loc[recovered_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "\n",
    "dead_obs =  np.array(\n",
    "    dead_cases.loc[dead_cases[\"Country/Region\"] == \"Germany\", \n",
    "                        date_formatted_begin:date_formatted_end])[0]\n",
    "\n",
    "data_germany = np.stack([cases_obs, recovered_obs, dead_obs]).T\n",
    "data_germany = np.diff(data_germany, axis=0)\n",
    "T_germany = data_germany.shape[0]\n",
    "N_germany = 83e6\n",
    "mean_g = np.mean(data_germany, axis=0)\n",
    "std_g = np.std(data_germany, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epidemiological Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     43,
     59
    ]
   },
   "outputs": [],
   "source": [
    "alpha_f = (0.7**2)*((1-0.7)/(0.17**2) - (1-0.7))\n",
    "beta_f = alpha_f*(1/0.7 - 1)\n",
    "\n",
    "\n",
    "def prior_sir():\n",
    "    \"\"\"\n",
    "    Implements batch sampling from a stationary prior over the parameters\n",
    "    of the non-stationary SIR model.\n",
    "    \"\"\"\n",
    "    \n",
    "    t1 = np.random.normal(loc=8, scale=3)\n",
    "    t2 = np.random.normal(loc=15, scale=1)\n",
    "    t3 = np.random.normal(loc=22, scale=1)\n",
    "    t4 = np.random.normal(loc=66, scale=1) \n",
    "    delta_t1 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    delta_t2 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    delta_t3 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    delta_t4 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
    "    lambd0 = np.random.lognormal(mean=np.log(1.2), sigma=0.5)\n",
    "    lambd1 = np.random.lognormal(mean=np.log(0.6), sigma=0.5)\n",
    "    lambd2 = np.random.lognormal(mean=np.log(0.3), sigma=0.5)\n",
    "    lambd3 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
    "    lambd4 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
    "    mu = np.random.lognormal(mean=np.log(1/8), sigma=0.2)\n",
    "    f_i = np.random.beta(a=alpha_f, b=beta_f)\n",
    "    phi_i = stats.vonmises(kappa=0.01).rvs()\n",
    "    f_r = np.random.beta(a=alpha_f, b=beta_f)\n",
    "    phi_r = stats.vonmises(kappa=0.01).rvs()\n",
    "    f_d = np.random.beta(a=alpha_f, b=beta_f)\n",
    "    phi_d = stats.vonmises(kappa=0.01).rvs()\n",
    "    D_i = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
    "    D_r = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
    "    D_d = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
    "    E0 = np.random.gamma(shape=2, scale=30)\n",
    "    scale_I = np.random.gamma(shape=1, scale=5)\n",
    "    scale_R = np.random.gamma(shape=1, scale=5)\n",
    "    scale_D = np.random.gamma(shape=1, scale=5)\n",
    "    return [t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, \n",
    "            lambd0, lambd1, lambd2, lambd3, lambd4, mu, \n",
    "            f_i, phi_i, f_r, phi_r, f_d, phi_d, \n",
    "            D_i, D_r, D_d, E0, scale_I, scale_R, scale_D]\n",
    "\n",
    "\n",
    "def prior_secir():\n",
    "    \"\"\"\n",
    "    Implements batch sampling from a stationary prior over the parameters\n",
    "    of the non-stationary SIR model.\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = np.random.uniform(low=0.005, high=0.9)\n",
    "    beta = np.random.lognormal(mean=np.log(0.25), sigma=0.3)\n",
    "    gamma = np.random.lognormal(mean=np.log(1/6.5), sigma=0.5)\n",
    "    eta = np.random.lognormal(mean=np.log(1/3.2), sigma=0.3)\n",
    "    theta = np.random.uniform(low=1/14, high=1/3)\n",
    "    delta = np.random.uniform(low=0.01, high=0.3)\n",
    "    d = np.random.uniform(low=1/14, high=1/3)\n",
    "    return [alpha, beta, gamma, eta, theta, delta, d]\n",
    "\n",
    "\n",
    "def calc_lambda_array(sim_lag, lambd0, lambd1, lambd2, lambd3, lambd4, \n",
    "                      t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, T):\n",
    "    \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
    "    \n",
    "    # Array of initial lambdas\n",
    "    lambd0_arr = np.array([lambd0] * (t1+sim_lag))\n",
    "    \n",
    "    # Compute lambd1 array\n",
    "    if delta_t1 == 1:\n",
    "        lambd1_arr = np.array([lambd1] * (t2-t1))\n",
    "    else:\n",
    "        lambd1_arr = np.linspace(lambd0, lambd1, delta_t1)\n",
    "        lambd1_arr = np.append(lambd1_arr, [lambd1] * (t2-t1-delta_t1))\n",
    "        \n",
    "    # Compute lambd2 array\n",
    "    if delta_t2 == 1:\n",
    "        lambd2_arr = np.array([lambd2] * (t3-t2))\n",
    "    else:\n",
    "        lambd2_arr = np.linspace(lambd1, lambd2, delta_t2)\n",
    "        lambd2_arr = np.append(lambd2_arr, [lambd2] * (t3-t2-delta_t2))\n",
    "        \n",
    "    # Compute lambd3 array\n",
    "    if delta_t3 == 1:\n",
    "        lambd3_arr = np.array([lambd3] * (t4-t3))\n",
    "    else:\n",
    "        lambd3_arr = np.linspace(lambd3, lambd4, delta_t3)\n",
    "        lambd3_arr = np.append(lambd3_arr, [lambd3] * (t4-t3-delta_t3))\n",
    "        \n",
    "    # Compute lambd4 array\n",
    "    if delta_t4 == 1:\n",
    "        lambd4_arr = np.array([lambd4] * (T-t4))\n",
    "    else:\n",
    "        lambd4_arr = np.linspace(lambd3, lambd4, delta_t4)\n",
    "        lambd4_arr = np.append(lambd4_arr, [lambd4] * (T-t4-delta_t4))\n",
    "    \n",
    "    return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
    "\n",
    "    \n",
    "def non_stationary_SEICR(params_sir, params_secir, N, T, sim_diff=16, observation_model=True):\n",
    "    \"\"\"\n",
    "    Performs a forward simulation from the stationary SIR model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract parameters \n",
    "    t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, lambd0, lambd1, lambd2, lambd3, lambd4, mu, f_i, phi_i, f_r, phi_r, f_d, phi_d, delay_i, delay_r, delay_d, E0, scale_I, scale_R, scale_D = params_sir\n",
    "    alpha, beta, gamma, eta, theta, delta, d = params_secir\n",
    "    \n",
    "    # Round integer parameters\n",
    "    t1, t2, t3, t4 = int(round(t1)), int(round(t2)), int(round(t3)), int(round(t4))\n",
    "    delta_t1, delta_t2, delta_t3, delta_t4 = int(round(delta_t1)), int(round(delta_t2)), int(round(delta_t3)), int(round(delta_t4))\n",
    "    E0 = max(1, np.round(E0)) \n",
    "    delay_i = int(round(delay_i)) \n",
    "    delay_r = int(round(delay_r)) \n",
    "    delay_d = int(round(delay_d)) \n",
    "    \n",
    "    # Impose constraints\n",
    "    assert sim_diff > delay_i\n",
    "    assert sim_diff > delay_r\n",
    "    assert sim_diff > delay_d\n",
    "    assert t1 > 0 and t2 > 0 and t3 > 0 and t4 > 0\n",
    "    assert t1 < t2 < t3 < t4\n",
    "    assert delta_t1 > 0 and delta_t2 > 0 and delta_t3 > 0 and delta_t4 > 0\n",
    "    assert t2 - t1 >= delta_t1 and t3 - t2 >= delta_t2 and t4-t3 >= delta_t3 and T-t4 >= delta_t4\n",
    "\n",
    "    # Calculate lambda arrays\n",
    "    # Lambda0 is the initial contact rate which will be consecutively\n",
    "    # reduced via the government measures\n",
    "    sim_lag = sim_diff - 1\n",
    "    lambd_arr = calc_lambda_array(sim_lag, lambd0, lambd1, lambd2, lambd3, lambd4, \n",
    "                                  t1, t2, t3, t4, delta_t1, delta_t2, delta_t3, delta_t4, T)\n",
    " \n",
    "    # Initial conditions\n",
    "    S, E, C, I, R, D = [N-E0], [E0], [0], [0], [0], [0]\n",
    "    \n",
    "    # Containers\n",
    "    I_news = []\n",
    "    R_news = []\n",
    "    D_news = []\n",
    "    \n",
    "    # Reported new cases\n",
    "    I_data = np.zeros(T)\n",
    "    R_data = np.zeros(T)\n",
    "    D_data = np.zeros(T)\n",
    "    fs_i = np.zeros(T)\n",
    "    fs_r = np.zeros(T)\n",
    "    fs_d = np.zeros(T)\n",
    " \n",
    "    # Simulate T-1 tiemsteps\n",
    "    for t in range(T+sim_lag):\n",
    "        \n",
    "        # Calculate new exposed cases\n",
    "        E_new = lambd_arr[t] * ((C[t] + beta*I[t])/N)*S[t]\n",
    "    \n",
    "        # Remove exposed from susceptible\n",
    "        S_t = S[t] - E_new\n",
    "        \n",
    "        # Calculate current exposed by adding new exposed and\n",
    "        # subtracting the exposed becoming carriers.\n",
    "        E_t = E[t] + E_new - gamma*E[t]\n",
    "        \n",
    "        # Calculate current carriers by adding the new exposed and subtracting\n",
    "        # those who will develop symptoms and become detected and those who\n",
    "        # will go through the disease asymptomatically.\n",
    "        C_t = C[t] + gamma*E[t] - (1-alpha)*eta*C[t] - alpha*theta*C[t]\n",
    "        \n",
    "        # Calculate current infected by adding the symptomatic carriers and \n",
    "        # subtracting the dead and recovered. The newly infected are just the \n",
    "        # carriers who get detected.\n",
    "        I_t = I[t] + (1-alpha)*eta*C[t] - (1-delta)*mu*I[t] - delta*d*I[t]\n",
    "        I_new = (1-alpha)*eta*C[t]\n",
    "        \n",
    "        # Calculate current recovered by adding the symptomatic and asymptomatic\n",
    "        # recovered. The newly recovered are only the detected recovered\n",
    "        R_t = R[t] + alpha*theta*C[t] + (1-delta)*mu*I[t]\n",
    "        R_new = (1-delta)*mu*I[t]\n",
    "        \n",
    "        # Calculate the current dead\n",
    "        D_t = D[t] + delta*d*I[t]\n",
    "        D_new = delta*d*I[t]\n",
    "        \n",
    "        # Ensure some numerical onstraints\n",
    "        S_t = np.clip(S_t, 0, N)\n",
    "        E_t = np.clip(E_t, 0, N)\n",
    "        C_t = np.clip(C_t, 0, N)\n",
    "        I_t = np.clip(I_t, 0, N)\n",
    "        R_t = np.clip(R_t, 0, N)\n",
    "        D_t = np.clip(D_t, 0, N)\n",
    "        \n",
    "        # Keep track of process over time\n",
    "        S.append(S_t)\n",
    "        E.append(E_t)\n",
    "        C.append(C_t)\n",
    "        I.append(I_t)\n",
    "        R.append(R_t)\n",
    "        D.append(D_t)\n",
    "        I_news.append(I_new)\n",
    "        R_news.append(R_new)\n",
    "        D_news.append(D_new)\n",
    "        \n",
    "        # From here, start adding new cases with delay D\n",
    "        # Note, we assume the same delay\n",
    "        if t >= sim_lag:\n",
    "            \n",
    "            # Compute lags and add to data arrays\n",
    "            fs_i[t-sim_lag] = (1-f_i)*(1 - np.abs( np.sin( (np.pi/7) * (t-sim_lag) - 0.5*phi_i)) )\n",
    "            fs_r[t-sim_lag] = (1-f_r)*(1 - np.abs( np.sin( (np.pi/7) * (t-sim_lag) - 0.5*phi_r)) )\n",
    "            fs_d[t-sim_lag] = (1-f_d)*(1 - np.abs( np.sin( (np.pi/7) * (t-sim_lag) - 0.5*phi_d)) )\n",
    "            I_data[t-sim_lag] = I_news[t-delay_i]\n",
    "            R_data[t-sim_lag] = R_news[t-delay_r]\n",
    "            D_data[t-sim_lag] = D_news[t-delay_d]\n",
    "            \n",
    "    # Compute weekly modulation\n",
    "    I_data = (1-fs_i) * I_data\n",
    "    R_data = (1-fs_r) * R_data\n",
    "    D_data = (1-fs_d) * D_data\n",
    "    \n",
    "    # Add noise\n",
    "    I_data = stats.t(df=4, loc=I_data, scale=np.sqrt(I_data)*scale_I).rvs()\n",
    "    R_data = stats.t(df=4, loc=R_data, scale=np.sqrt(R_data)*scale_R).rvs()\n",
    "    D_data = stats.t(df=4, loc=D_data, scale=np.sqrt(D_data)*scale_D).rvs()\n",
    "    \n",
    "    if observation_model:\n",
    "        return np.stack((I_data, R_data, D_data)).T\n",
    "    return np.stack((S, E, I, C, R, D)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def data_generator(n_sim, n_obs, N=None, sim_diff=21, N_min=10000, N_max=70000000):\n",
    "    \"\"\"\n",
    "    Runs the forward model 'batch_size' times by first sampling fromt the prior\n",
    "    theta ~ p(theta) and running x ~ p(x|theta).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Variable size N\n",
    "    if N is None:\n",
    "        N = np.random.randint(N_min, N_max)\n",
    "        \n",
    "    # Generate data\n",
    "    # x is a np.ndarray of shape (batch_size, n_obs, x_dim)\n",
    "    x = []\n",
    "    theta = []\n",
    "    for i in range(n_sim):\n",
    "        \n",
    "        # Reject meaningless simulaitons\n",
    "        x_i = None\n",
    "        while x_i is None:\n",
    "            try:\n",
    "                theta1 = prior_sir()\n",
    "                theta2 = prior_secir()\n",
    "                x_i = non_stationary_SEICR(theta1, theta2, N, n_obs, sim_diff=sim_diff)\n",
    "                x_i = (x_i - mean_g) / std_g\n",
    "            except:\n",
    "                 pass\n",
    "        # Simulate SECIR\n",
    "        x.append(x_i)\n",
    "        theta.append(theta1 + theta2)\n",
    "    x = np.array(x)\n",
    "    theta = np.array(theta)\n",
    "\n",
    "    # Convert to tensor, if specified \n",
    "    return theta.astype(np.float32), x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [r'$t_1$', r'$t_2$', r'$t_3$', r'$t_4$',\n",
    "               r'$\\Delta t_1$', r'$\\Delta t_2$', r'$\\Delta t_3$', r'$\\Delta t_4$',\n",
    "               r'$\\lambda_0$', r'$\\lambda_1$', r'$\\lambda_2$', r'$\\lambda_3$', r'$\\lambda_4$', \n",
    "               r'$\\mu$', r'$f_I$', r'$\\phi_I$',  r'$f_R$', r'$\\phi_R$',  \n",
    "               r'$f_D$', r'$\\phi_D$',\n",
    "               r'$L_I$', r'$L_R$', r'$L_D$', r'$E_0$', r'$\\sigma_I$', r'$\\sigma_R$', r'$\\sigma_D$', \n",
    "               r'$\\alpha$', r'$\\beta$', r'$\\gamma$',\n",
    "               r'$\\eta$', r'$\\theta$', r'$\\delta$', r'$d$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = partial(data_generator, N=N_germany, sim_diff=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "class MultiConvLayer(tf.keras.Model):\n",
    "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
    "    def __init__(self, n_filters=32, strides=1):\n",
    "        super(MultiConvLayer, self).__init__()\n",
    "        \n",
    "        self.convs = [\n",
    "            Conv1D(n_filters//2, kernel_size=f, strides=strides, \n",
    "                                   padding='causal', activation='relu', kernel_initializer='glorot_uniform')\n",
    "            for f in range(2, 8)\n",
    "        ]\n",
    "        self.dim_red = Conv1D(n_filters, 1, 1, activation='relu', kernel_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
    "        \n",
    "        out = tf.concat([conv(x) for conv in self.convs], axis=-1)\n",
    "        out = self.dim_red(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class MultiConvNet(tf.keras.Model):\n",
    "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
    "    def __init__(self, n_layers=3, n_filters=64, strides=1):\n",
    "        super(MultiConvNet, self).__init__()\n",
    "        \n",
    "        self.net = Sequential([\n",
    "            MultiConvLayer(n_filters, strides)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.lstm = LSTM(n_filters)\n",
    "        \n",
    "    def call(self, x, **args):\n",
    "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
    "        \n",
    "        out = self.net(x)\n",
    "        out = self.lstm(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class SummaryNet(tf.keras.Model):\n",
    "    def __init__(self, n_summary=192):\n",
    "        super(SummaryNet, self).__init__()\n",
    "        self.net_I = MultiConvNet(n_filters=n_summary//3)\n",
    "        self.net_R = MultiConvNet(n_filters=n_summary//3)\n",
    "        self.net_D = MultiConvNet(n_filters=n_summary//3)\n",
    "    \n",
    "    def call(self, x, **args):\n",
    "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
    "        \n",
    "        x = tf.split(x, 3, axis=-1)\n",
    "        x_i = self.net_I(x[0])\n",
    "        x_r = self.net_R(x[1])\n",
    "        x_d = self.net_D(x[2])\n",
    "        return tf.concat([x_i, x_r, x_d], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict={\n",
    "    'n_coupling_layers': 6,\n",
    "    's_args': {\n",
    "        'units': [192, 192, 192],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [192, 192, 192],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'alpha': 1.9,\n",
    "    'use_permutation': True,\n",
    "    'use_act_norm': True,\n",
    "    'n_params': len(param_names),\n",
    "}\n",
    "\n",
    "\n",
    "summary_net = SummaryNet()\n",
    "inference_net = InvertibleNetwork(meta_dict)\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_learning_rate = 0.0005\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    starter_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.99,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ParameterEstimationTrainer(\n",
    "    network=amortizer, \n",
    "    generative_model=data_gen,\n",
    "    loss=mmd_kl_loss,\n",
    "    learning_rate=learning_rate,\n",
    "    checkpoint_path=f'export_ckpt/mmd/covid19',\n",
    "    max_to_keep=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#losses = trainer.train_online(epochs=50, iterations_per_epoch=1000, batch_size=64, n_obs=T_germany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3\n",
    "try:\n",
    "    assert a > 5, 'Failed condition'\n",
    "    print('fddfd')\n",
    "except AssertionError as err:\n",
    "    print(err)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._forward_inference(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
