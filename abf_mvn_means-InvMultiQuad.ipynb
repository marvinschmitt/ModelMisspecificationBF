{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project: Adversarial BayesFlow - Experiment 1: MVN means\n",
    "**Task:** Learn means of a 2-variate Gaussian with unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "import colorednoise as cn\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../BayesFlow')))\n",
    "\n",
    "from bayesflow.trainers import ParameterEstimationTrainer\n",
    "from bayesflow.networks import InvertibleNetwork, InvariantNetwork\n",
    "from bayesflow.amortizers import SingleModelAmortizer\n",
    "from bayesflow.models import GenerativeModel\n",
    "from bayesflow.exceptions import ConfigurationError\n",
    "from bayesflow.applications.priors import GaussianMeanPrior, TPrior, GaussianMeanCovPrior\n",
    "from bayesflow.applications.simulators import GaussianMeanSimulator, MultivariateTSimulator, GaussianMeanCovSimulator\n",
    "\n",
    "from abf_functions import *\n",
    "\n",
    "\n",
    "RERUN_GRID_EXPERIMENT = True\n",
    "RERUN_MMD_ERROR_EXPERIMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\" : 20,\n",
    "    \"xtick.labelsize\" : 16,\n",
    "    \"ytick.labelsize\" : 16,\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"times\"],\n",
    "    'text.latex.preamble' : r'\\usepackage{{amsmath}}'\n",
    "})\n",
    "\n",
    "FILEFORMAT = 'pdf'\n",
    "DPI = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: \n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "S_sufficient = 2\n",
    "S_overcomplete = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Color codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors = sns.color_palette(\"viridis\", n_colors=6)\n",
    "colors = build_viridis_palette(20, base_palette=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAABICAYAAADI6S+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+klEQVR4nO3ZMWpUURiG4XMkgkg0TQYLi3TBBdikCLgUN+AmArGztbBxQ7YSpglIQiAwnUWiKX57E8ELc/xmrs9TXi7D9zdvcadXVQPg33uUHgDwvxJggBABBggRYIAQAQYIEWCAkJ0pLz/tz2uvLUZtiXt5eJWeMNSP1W56wjB3Px+nJwz1ZPcmPWGo5fWz9IShbutiVVX34jkpwHtt0d729+tbtWFOP56kJwx1/vk4PWGY68sX6QlDvTr6mp4w1PGHN+kJQ53dvfv20HOfIABCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIKRX1d+/3Pv31tpy3Jy4/dbaKj1ikDnf1pr7tt3c7zuoqsXvD3cm/siyql6vadDG6b1/met9c76tNfdtu7nf9yc+QQCECDBAyNQAfxqyYnPM+b4539aa+7bd3O970KQ/4QBYH58gAEIEGCBEgAFCBBggRIABQn4Bj5pK1/6qbMsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color_codes = {\n",
    "    \"Prior location\" : colors[4],\n",
    "    \"Prior scale\" : colors[17],\n",
    "    \"Prior location + scale\" : colors[11],\n",
    "    \"Simulator\" : colors[9],\n",
    "    \"Noise\" : colors[13],\n",
    "    \"No MMS\" : colors[0]\n",
    "}\n",
    "sns.palplot(color_codes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sufficient and overcomplete BayesFlow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing networks from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Sufficient\n",
    "\n",
    "#########\n",
    "\n",
    "prior = GaussianMeanPrior(D=D)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "#########\n",
    "\n",
    "summary_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    1,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 32},\n",
    "}\n",
    "\n",
    "class BottleneckSummaryNet(tf.keras.Model):\n",
    "    def __init__(self, inv_meta={}, n_out=10, activation_out=None):\n",
    "        super(BottleneckSummaryNet, self).__init__()\n",
    "\n",
    "        self.invariant_net = InvariantNetwork(inv_meta)\n",
    "        self.out_layer = tf.keras.layers.Dense(n_out, activation=activation_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out_inv = self.invariant_net(x)\n",
    "        out = self.out_layer(out_inv)\n",
    "        return out\n",
    "\n",
    "\n",
    "summary_net = BottleneckSummaryNet(inv_meta=summary_meta, \n",
    "                                   n_out=S_sufficient,\n",
    "                                   activation_out=None  # linear\n",
    "                                  )\n",
    "\n",
    "\n",
    "inference_meta = {\n",
    "    'n_coupling_layers': 2,\n",
    "    's_args': {\n",
    "        'units': [32, 32, 32],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [32, 32, 32],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'n_params': D,\n",
    "    'alpha': 1.9,\n",
    "    'permute': True\n",
    "}\n",
    "\n",
    "inference_net = InvertibleNetwork(inference_meta)\n",
    "\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)\n",
    "\n",
    "trainer_sufficient = ParameterEstimationTrainer(amortizer,\n",
    "                      generative_model,\n",
    "                      loss=mmd_kl_loss,#partial(mmd_kl_loss, kernel = \"inverse_multiquadratic\"),\n",
    "                      learning_rate=0.0007,\n",
    "                      checkpoint_path=f'export_ckpt/mmd_inverse_quadratic/means_{D}D_sufficient',\n",
    "                      max_to_keep = 1\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing networks from scratch.\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "\n",
    "prior = GaussianMeanPrior(D=D)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "\n",
    "#########\n",
    "\n",
    "summary_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    1,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 32},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 32},\n",
    "}\n",
    "\n",
    "class BottleneckSummaryNet(tf.keras.Model):\n",
    "    def __init__(self, inv_meta={}, n_out=10, activation_out=None):\n",
    "        super(BottleneckSummaryNet, self).__init__()\n",
    "\n",
    "        self.invariant_net = InvariantNetwork(inv_meta)\n",
    "        self.out_layer = tf.keras.layers.Dense(n_out, activation=activation_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out_inv = self.invariant_net(x)\n",
    "        out = self.out_layer(out_inv)\n",
    "        return out\n",
    "\n",
    "\n",
    "summary_net = BottleneckSummaryNet(inv_meta=summary_meta, \n",
    "                                   n_out=S_overcomplete,\n",
    "                                   activation_out=None  # linear\n",
    "                                  )\n",
    "\n",
    "\n",
    "inference_meta = {\n",
    "    'n_coupling_layers': 2,\n",
    "    's_args': {\n",
    "        'units': [32, 32, 32],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [32, 32, 32],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'n_params': D,\n",
    "    'alpha': 1.9,\n",
    "    'permute': True\n",
    "}\n",
    "\n",
    "inference_net = InvertibleNetwork(inference_meta)\n",
    "\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)\n",
    "\n",
    "trainer_overcomplete = ParameterEstimationTrainer(amortizer,\n",
    "                      generative_model,\n",
    "                      loss=partial(mmd_kl_loss, kernel = \"inverse_multiquadratic\"),\n",
    "                      learning_rate=0.0007,\n",
    "                      checkpoint_path=f'export_ckpt/mmd_inverse_quadratic/means_{D}D_overcomplete',\n",
    "                      max_to_keep = 1\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converge BayesFlow networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating initial 10000 data sets...\n",
      "Converting 10000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2955c83d5e7c4d638ff7cb9c13f73268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17677a90ef8c4694b6d9d46c1a5abb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 2:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aca3970bf54495390d3b462d7140a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 3:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c287a534f049918de4f11fe530bfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 4:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110a030099784b8cb9c97dd5a10e4ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0df82a752ab4a3989606155c409e9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 6:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93b37f1101415b96988f995543f097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 7:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882d4eddfaf64a0aa70dcc94f8e7d1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 8:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1389b4f387de4d06bf48538a4a3771d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 9:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631eb9226bd74b27a37451d0230889dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 10:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating new 10000 data sets and appending to previous...\n",
      "New total number of simulated data sets: 20000\n",
      "Converting 20000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ee3e01f75c4e9db9576193f6fba919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea810352f9c141aa918448c6791813a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 2:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af54a3e75b845cb9ffb720bf0262141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 3:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4d433f7f594736bf4e8082145615f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 4:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a22467930a7414f805c8cd79d7f506c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 5:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6805b2df338b45bca4dc9a9c0b9ed963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 6:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e6c830b620453fa43113ca197eeb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 7:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93248c0491d64755a2b3f0f5fc685317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 8:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7215c1896a8466ea5b290f0cc8443ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 9:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693bbe58c2e448a0af976029dbfb3f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 10:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating new 10000 data sets and appending to previous...\n",
      "New total number of simulated data sets: 30000\n",
      "Converting 30000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047321a5b665434eaa6ad76d277cb091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af69c6beb71462fb0aaf45b7b6a9f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 2:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c39b98c3d28423390298fe83a757653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 3:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e728d757e8c4502bc0978717ed44127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 4:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2996a8dc19bb4ec2937ea2befa3c36b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 5:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bfbec374a34e2db00dc498d7eb826d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 6:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c325c5e4b24f92a3da8b27fed34894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 7:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c09142cbcc54a99b20635c9aff3fe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 8:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3043e9ca1a3b4833bada2b72589268fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 9:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c1819689b1499bbac79a2dcdcc8f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 10:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating new 10000 data sets and appending to previous...\n",
      "New total number of simulated data sets: 40000\n",
      "Converting 40000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b4185dfff489bb06df3fa45ef0195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fded41a59f9947259cbb6b6b042a6c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ce44585ae341b48c7f557b7ce91417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 3:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62bb184f7024c6598b66f7961c66a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 4:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf6022361ea45509862b4d3a0a09138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 5:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cb6e77c00a4e82bd62f636c54c5a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 6:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251ea08dd258455aba50cb22ab778f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 7:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7b52f6e59044a397be5790c138d356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 8:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1780dc5607f4a9890d98070c37711bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 9:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700263031f9b4ca89acddaeb6bfcedd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 10:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating new 10000 data sets and appending to previous...\n",
      "New total number of simulated data sets: 50000\n",
      "Converting 50000 simulations to a TensorFlow data set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81f824d72ea4bdeb61d810e7ba22d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7269c4a97a164b92a99aad4923ed68e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = trainer_sufficient.train_rounds(epochs=10, rounds=5, sim_per_round=10000, batch_size=128, n_obs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainer_overcomplete.train_rounds(epochs=10, rounds=5, sim_per_round=10000, batch_size=128, n_obs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance: Recover analytic Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_analytic_posterior(prior, simulator, x):\n",
    "    n_sim, n_obs, D = x.shape\n",
    "    \n",
    "    # Set up variables\n",
    "    x_bar = np.mean(x, axis=1)                 # empirical mean\n",
    "    sigma_0 = np.eye(D) * prior.mu_scale       # mu prior covariance\n",
    "    sigma_0_inv = np.linalg.inv(sigma_0)       # inverse mu prior covariance\n",
    "    mu_0 = np.ones((D, 1)) * prior.mu_mean     # mu prior mean\n",
    "    sigma = simulator.sigma                    # likelihood covariance\n",
    "    sigma_inv = np.linalg.inv(sigma)           # inverse likelihood covariance\n",
    "    \n",
    "    mu_posterior_covariance = np.stack([np.linalg.inv(sigma_0_inv + n_obs*sigma_inv)] * n_sim)\n",
    "    \n",
    "    mu_posterior_mean = mu_posterior_covariance @ (sigma_0_inv @ mu_0 + n_obs * (sigma_inv @ x_bar[..., np.newaxis]))   \n",
    "    mu_posterior_mean = mu_posterior_mean.reshape(n_sim, D)\n",
    "\n",
    "    return mu_posterior_mean, mu_posterior_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_diagnostics(trainer, generative_model, theta=None, x=None, filename=None):\n",
    "    theta, x = generative_model(200, 100) if theta is None and x is None else (theta, x)\n",
    "    param_samples = trainer.network.sample(x, n_samples=200)\n",
    "    param_means = param_samples.mean(axis=0)\n",
    "\n",
    "    # analytic posteriors\n",
    "    print(\"\\n\\nBayesFlow (x) vs. analytic posterior means (y) -- Recovery of analytic posterior means\")\n",
    "    prior = trainer.generative_model.prior.__self__\n",
    "    simulator = trainer.generative_model.simulator\n",
    "    posterior_means, posterior_covariances = calculate_analytic_posterior(prior, simulator, x)\n",
    "    posterior_variances = posterior_covariances.diagonal(axis1=1, axis2=2)\n",
    "    \n",
    "    true_vs_estimated(posterior_means, param_means, [r'$\\mu_%i$'%i for i in range(1, D+1)], figsize=(8,4), dpi=DPI,\n",
    "                          filename=f\"{filename}_true_analytic.{FILEFORMAT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: $\\mathcal{M}_{obs} = \\mathcal{M}^*$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200,100)\n",
    "\n",
    "adversarial_diagnostics(trainer_sufficient, generative_model, \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_sufficient_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200,100)\n",
    "\n",
    "adversarial_diagnostics(trainer_overcomplete, generative_model, \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_overcomplete_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrong Prior\n",
    "The prior over the multivariate Gaussian's means is Gaussian: $\\mu\\sim\\mathcal{N}(\\mu|\\mu_0, \\Sigma_0)$\n",
    "\n",
    "- Wrong (free) prior location and scale: $\\mu\\sim\\mathcal{N}(\\mu_0, \\tau_0\\cdot\\mathbb{I})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = 5.0\n",
    "tau_0 = 2.5\n",
    "\n",
    "prior = GaussianMeanPrior(D=D, mu_mean=mu_0, mu_scale=tau_0)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200,100)\n",
    "\n",
    "adversarial_diagnostics(trainer_sufficient, generative_model,  \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_sufficient_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = 5.0\n",
    "tau_0 = 2.5\n",
    "\n",
    "prior = GaussianMeanPrior(D=D, mu_mean=mu_0, mu_scale=tau_0)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200,100)\n",
    "\n",
    "adversarial_diagnostics(trainer_overcomplete, generative_model,  \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_overcomplete_prior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A2) Wrong Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{x}_k\\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\tau\\cdot\\mathbb{I})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 10.0\n",
    "\n",
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D, s = tau)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200,100)\n",
    "\n",
    "adversarial_diagnostics(trainer_sufficient, generative_model,  \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_sufficient_likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 10.0\n",
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D, s = tau)\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200,100)\n",
    "\n",
    "adversarial_diagnostics(trainer_overcomplete, generative_model,  \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_overcomplete_likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A3) Noise\n",
    "$\\mathbf{x}_k \\sim \\lambda\\cdot\\mathrm{Beta}(2,5)+(1-\\lambda)\\cdot\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$\n",
    "\n",
    "In this scenario, the contamination is a mixture of Beta and Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200, 100)\n",
    "\n",
    "lamda = 0.5  # contribution of contamination xi \n",
    "\n",
    "x = noisify_x(x, lamda = lamda,\n",
    "                      noise_sampler = partial(beta_noise_sampler, a=2, b=5, tau=1))\n",
    "\n",
    "adversarial_diagnostics(trainer_sufficient, generative_model=None, theta=theta, x=x,  \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_sufficient_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "simulator = GaussianMeanSimulator(D=D)\n",
    "\n",
    "generative_model = GenerativeModel(prior, simulator)\n",
    "theta, x = generative_model(200, 100)\n",
    "\n",
    "lamda = 0.5  # contribution of contamination xi \n",
    "\n",
    "x = noisify_x(x, lamda = lamda,\n",
    "                      noise_sampler = partial(beta_noise_sampler, a=2, b=5, tau=1))\n",
    "\n",
    "adversarial_diagnostics(trainer_overcomplete, generative_model=None, theta=theta, x=x,  \n",
    "                        #print_plots=True,\n",
    "                       filename=f\"plots/abf_mvn_means_overcomplete_noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary space: MMD contour plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\widehat{rMMD}$ w.r.t. prior (location and scale factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RERUN_GRID_EXPERIMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_loc = 101\n",
    "n_scale = 101\n",
    "prior_loc = np.linspace(-6.0, 6.0, num = n_loc)\n",
    "prior_scale = np.linspace(0.10, 5.90, num = n_scale)\n",
    "P1, P2 = np.meshgrid(prior_loc, prior_scale)\n",
    "\n",
    "if RERUN_GRID_EXPERIMENT:\n",
    "    _, x_star = trainer_sufficient._forward_inference(200, 100)\n",
    "    s_star = np.array(trainer_sufficient.network.summary_net(x_star))\n",
    "\n",
    "    MMD = np.zeros((n_scale, n_loc))\n",
    "    for i in tqdm(range(n_scale)):\n",
    "        for j in range(n_loc):\n",
    "            p1 = P1[i, j]\n",
    "            p2 = P2[i, j]\n",
    "            prior = GaussianMeanPrior(D=D, mu_mean=p1, mu_scale=p2)\n",
    "            simulator = GaussianMeanSimulator(D=D)\n",
    "            generative_model = GenerativeModel(prior, simulator)\n",
    "            _, x_o = generative_model(200, 100)\n",
    "            s_o = np.array(trainer_sufficient.network.summary_net(x_o))\n",
    "            MMD[i, j] = float(maximum_mean_discrepancy(s_o, s_star, squared=False))\n",
    "    np.save(f\"data/MMD_grid_experiments/MVN_MMD_grid_sufficient_prior.npy\", MMD)\n",
    "    \n",
    "MMD = np.load(f\"data/MMD_grid_experiments/MVN_MMD_grid_sufficient_prior.npy\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.pcolor(P1, P2, MMD, shading=\"nearest\", rasterized=True)\n",
    "plt.xlabel(r\"$\\mu_0$\")\n",
    "plt.ylabel(r\"$\\tau_0$ (scale factor of $\\Sigma_0$)\")\n",
    "plt.axhline(y=1.0, linestyle=\"--\", color=\"lightgreen\", alpha=.80)\n",
    "plt.axvline(x=0.0, linestyle=\"--\", color=\"lightgreen\", alpha=.80)\n",
    "\n",
    "plt.xticks([-5, 0, 5])\n",
    "\n",
    "plt.plot(5, 2.5, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Prior location + scale\"]\n",
    "        )\n",
    "plt.plot(0, 2.5, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Prior scale\"]\n",
    "        )\n",
    "plt.plot(5, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Prior location\"]\n",
    "        )\n",
    "\n",
    "plt.plot(0, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"No MMS\"]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "plt.savefig(f\"plots/abf_mvn_means_sufficient_mmd_grid_prior.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_loc = 101\n",
    "n_scale = 101\n",
    "prior_loc = np.linspace(-6.0, 6.0, num = n_loc)\n",
    "prior_scale = np.linspace(0.10, 5.90, num = n_scale)\n",
    "P1, P2 = np.meshgrid(prior_loc, prior_scale)\n",
    "\n",
    "if RERUN_GRID_EXPERIMENT:\n",
    "    _, x_star = trainer_overcomplete._forward_inference(200, 100)\n",
    "    s_star = np.array(trainer_overcomplete.network.summary_net(x_star))\n",
    "\n",
    "    MMD = np.zeros((n_scale, n_loc))\n",
    "    for i in tqdm(range(n_scale)):\n",
    "        for j in range(n_loc):\n",
    "            p1 = P1[i, j]\n",
    "            p2 = P2[i, j]\n",
    "            prior = GaussianMeanPrior(D=D, mu_mean=p1, mu_scale=p2)\n",
    "            simulator = GaussianMeanSimulator(D=D)\n",
    "            generative_model = GenerativeModel(prior, simulator)\n",
    "            _, x_o = generative_model(200, 100)\n",
    "            s_o = np.array(trainer_overcomplete.network.summary_net(x_o))\n",
    "            MMD[i, j] = float(maximum_mean_discrepancy(s_o, s_star, squared=False))\n",
    "    np.save(f\"data/MMD_grid_experiments/MVN_MMD_grid_overcomplete_prior.npy\", MMD)\n",
    "    \n",
    "MMD = np.load(f\"data/MMD_grid_experiments/MVN_MMD_grid_overcomplete_prior.npy\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.pcolor(P1, P2, MMD, shading=\"nearest\", rasterized=True)\n",
    "plt.xlabel(r\"$\\mu_0$\")\n",
    "plt.ylabel(r\"$\\tau_0$ (scale factor of $\\Sigma_0$)\")\n",
    "plt.axhline(y=1.0, linestyle=\"--\", color=\"lightgreen\", alpha=.80)\n",
    "plt.axvline(x=0.0, linestyle=\"--\", color=\"lightgreen\", alpha=.80)\n",
    "\n",
    "plt.xticks([-5, 0, 5])\n",
    "\n",
    "plt.plot(5, 2.5, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Prior location + scale\"]\n",
    "        )\n",
    "plt.plot(0, 2.5, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Prior scale\"]\n",
    "        )\n",
    "plt.plot(5, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Prior location\"]\n",
    "        )\n",
    "\n",
    "plt.plot(0, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"No MMS\"]\n",
    "        )\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "plt.savefig(f\"plots/abf_mvn_means_overcomplete_mmd_grid_prior.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\widehat{rMMD}$ w.r.t. Simulator and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.markers import MarkerStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tau = 101\n",
    "n_lamda = 101\n",
    "tau_list = np.linspace(1.0, 21.0, num = n_tau)\n",
    "lamda_list = np.linspace(0.0, 1.0, num = n_lamda)\n",
    "\n",
    "tau_grid, lamda_grid = np.meshgrid(tau_list, lamda_list)\n",
    "\n",
    "if RERUN_GRID_EXPERIMENT:\n",
    "    _, x = trainer_sufficient._forward_inference(200, 100)\n",
    "    z_psi = np.array(trainer_sufficient.network.summary_net(x))\n",
    "\n",
    "    MMD = np.zeros((n_lamda, n_tau))\n",
    "    for i in tqdm(range(n_lamda)):\n",
    "        for j in range(n_tau):\n",
    "            tau = tau_grid[i, j]\n",
    "            lamda = lamda_grid[i, j]\n",
    "            prior = GaussianMeanPrior(D=D, mu_mean=0.0, mu_scale=1.0)\n",
    "            simulator = GaussianMeanSimulator(D=D, s=tau)\n",
    "            generative_model = GenerativeModel(prior, simulator)\n",
    "            _, x_o = generative_model(200, 100)\n",
    "\n",
    "            x_o = noisify_x(x_o, lamda = lamda,\n",
    "                      noise_sampler = partial(beta_noise_sampler, a=2, b=5, tau=tau))\n",
    "\n",
    "            s_o = np.array(trainer_sufficient.network.summary_net(x_o))\n",
    "            MMD[i, j] = float(maximum_mean_discrepancy(s_o, z_psi, squared=False))\n",
    "    np.save(f\"data/MMD_grid_experiments/MVN_MMD_grid_sufficient_likelihood_noise.npy\", MMD)\n",
    "    \n",
    "\n",
    "\n",
    "MMD = np.load(f\"data/MMD_grid_experiments/MVN_MMD_grid_sufficient_likelihood_noise.npy\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.pcolor(lamda_list, tau_list, MMD.T, shading=\"nearest\", rasterized=True, vmin=0, vmax=3)\n",
    "\n",
    "plt.xlabel(r\"$\\lambda$ (noise fraction)\")\n",
    "plt.ylabel(r\"$\\tau$ (scale factor of $\\Sigma$)\")\n",
    "\n",
    "plt.axvline(x=0.0, linestyle=\"--\", color=\"lightgreen\", alpha=1.00, linewidth=2)\n",
    "plt.axhline(y=1.0, linestyle=\"--\", color=\"lightgreen\", alpha=1.00, linewidth=2)\n",
    "\n",
    "\n",
    "plt.plot(0.5, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Noise\"]\n",
    "        )\n",
    "plt.plot(0, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"No MMS\"]\n",
    "        )\n",
    "plt.plot(0, 10, linewidth=10, alpha=1.0, \n",
    "         marker=MarkerStyle(\"o\"), markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Simulator\"]\n",
    "        )\n",
    "\n",
    "\n",
    "plt.xlim(-0.03)\n",
    "plt.ylim(0)\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "\n",
    "plt.savefig(f\"plots/abf_mvn_means_sufficient_mmd_grid_likelihood_noise.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tau = 101\n",
    "n_lamda = 101\n",
    "tau_list = np.linspace(1.0, 21.0, num = n_tau)\n",
    "lamda_list = np.linspace(0.0, 1.0, num = n_lamda)\n",
    "\n",
    "tau_grid, lamda_grid = np.meshgrid(tau_list, lamda_list)\n",
    "\n",
    "if RERUN_GRID_EXPERIMENT:\n",
    "    _, x = trainer_overcomplete._forward_inference(200, 100)\n",
    "    z_psi = np.array(trainer_overcomplete.network.summary_net(x))\n",
    "\n",
    "    MMD = np.zeros((n_lamda, n_tau))\n",
    "    for i in tqdm(range(n_lamda)):\n",
    "        for j in range(n_tau):\n",
    "            tau = tau_grid[i, j]\n",
    "            lamda = lamda_grid[i, j]\n",
    "            prior = GaussianMeanPrior(D=D, mu_mean=0.0, mu_scale=1.0)\n",
    "            simulator = GaussianMeanSimulator(D=D, s=tau)\n",
    "            generative_model = GenerativeModel(prior, simulator)\n",
    "            _, x_o = generative_model(200, 100)\n",
    "\n",
    "            x_o = noisify_x(x_o, lamda = lamda,\n",
    "                      noise_sampler = partial(beta_noise_sampler, a=2, b=5, tau=tau))\n",
    "            \n",
    "            s_o = np.array(trainer_overcomplete.network.summary_net(x_o))\n",
    "            MMD[i, j] = float(maximum_mean_discrepancy(s_o, z_psi, squared=False))\n",
    "    np.save(f\"data/MMD_grid_experiments/MVN_MMD_grid_overcomplete_likelihood_noise.npy\", MMD)\n",
    "    \n",
    "\n",
    "\n",
    "MMD = np.load(f\"data/MMD_grid_experiments/MVN_MMD_grid_overcomplete_likelihood_noise.npy\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.pcolor(lamda_list, tau_list, MMD.T, shading=\"nearest\", rasterized=True, vmin=0, vmax=3)\n",
    "\n",
    "plt.xlabel(r\"$\\lambda$ (noise fraction)\")\n",
    "plt.ylabel(r\"$\\tau$ (scale factor of $\\Sigma$)\")\n",
    "\n",
    "plt.axvline(x=0.0, linestyle=\"--\", color=\"lightgreen\", alpha=1.00, linewidth=2)\n",
    "plt.axhline(y=1.0, linestyle=\"--\", color=\"lightgreen\", alpha=1.00, linewidth=2)\n",
    "\n",
    "\n",
    "plt.plot(0.5, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Noise\"]\n",
    "        )\n",
    "plt.plot(0, 1, linewidth=10, alpha=1.0, \n",
    "         marker=\"o\", markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"No MMS\"]\n",
    "        )\n",
    "plt.plot(0, 10, linewidth=10, alpha=1.0, \n",
    "         marker=MarkerStyle(\"o\"), markersize=20, markeredgewidth=5.0, markeredgecolor=\"white\", markerfacecolor = color_codes[\"Simulator\"]\n",
    "        )\n",
    "\n",
    "plt.xlim(-0.03)\n",
    "plt.ylim(0)\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "\n",
    "plt.savefig(f\"plots/abf_mvn_means_overcomplete_mmd_grid_likelihood_noise.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Space Pairplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplot sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = 10000\n",
    "\n",
    "# No MMS\n",
    "_, x_no_MMS = GenerativeModel(\n",
    "    GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1), \n",
    "    GaussianMeanSimulator(D=D))(n_sim, 100)\n",
    "\n",
    "# Prior location MMS\n",
    "_, x_prior_location_MMS = GenerativeModel(\n",
    "    GaussianMeanPrior(D=D, mu_mean=5, mu_scale=1), \n",
    "    GaussianMeanSimulator(D=D))(n_sim, 100)\n",
    "\n",
    "# Prior scale MMS\n",
    "_, x_prior_scale_MMS = GenerativeModel(\n",
    "    GaussianMeanPrior(D=D, mu_mean=0, mu_scale=2.5), \n",
    "    GaussianMeanSimulator(D=D))(n_sim, 100)\n",
    "\n",
    "# Prior location and scale MMS\n",
    "_, x_prior_location_scale_MMS = GenerativeModel(\n",
    "    GaussianMeanPrior(D=D, mu_mean=5, mu_scale=2.5), \n",
    "    GaussianMeanSimulator(D=D))(n_sim, 100)\n",
    "\n",
    "\n",
    "s_no_MMS = np.array(trainer_sufficient.network.summary_net(x_no_MMS))\n",
    "s_prior_location_MMS = np.array(trainer_sufficient.network.summary_net(x_prior_location_MMS))\n",
    "s_prior_scale_MMS = np.array(trainer_sufficient.network.summary_net(x_prior_scale_MMS))\n",
    "s_prior_location_scale_MMS = np.array(trainer_sufficient.network.summary_net(x_prior_location_scale_MMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAMES = ['Prior location', 'Prior scale', 'Prior location + scale', 'No MMS']\n",
    "S_ALL = [s_prior_location_MMS, s_prior_scale_MMS, s_prior_location_scale_MMS, s_no_MMS]\n",
    "DF = (pd.DataFrame(s, columns=[r'$s_{%i}$'%i for i in range(1, S_sufficient+1)]) for s in S_ALL)\n",
    "\n",
    "df = pd.concat(DF,\n",
    "              keys=TASK_NAMES,\n",
    "              names=['MMS', None]\n",
    "              ).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset(x, y, hue, mask, **kws):\n",
    "    sns.kdeplot(x=x[mask], y=y[mask], hue=hue[mask], fill=True, levels=10, antialiased=True, **kws)\n",
    "\n",
    "g = sns.PairGrid(df, hue=\"MMS\", height=3, palette=[color_codes[task_name] for task_name in TASK_NAMES] )\n",
    "\n",
    "g.map_diag(sns.kdeplot, linewidth=2)\n",
    "\n",
    "g.map_lower(plot_subset, mask= \n",
    "            (\n",
    "                (df[\"MMS\"] == 'Prior location') |\n",
    "                (df[\"MMS\"] == 'Prior scale') |\n",
    "                (df[\"MMS\"] == 'No MMS')\n",
    "           )\n",
    "           )\n",
    "\n",
    "\n",
    "g.map_upper(plot_subset, mask= \n",
    "            (\n",
    "                (df[\"MMS\"] == 'Prior location + scale') |\n",
    "                (df[\"MMS\"] == 'No MMS')\n",
    "           )\n",
    "           )\n",
    "\n",
    "#handles = [mpatches.Patch(facecolor=color_codes[\"No MMS\"], label=\"No MMS\"),\n",
    "#           mpatches.Patch(facecolor=color_codes[\"Prior location\"], label=\"Prior location\"),\n",
    "#           mpatches.Patch(facecolor=color_codes[\"Prior scale\"], label=r\"Prior scale\"),\n",
    "#           mpatches.Patch(facecolor=color_codes[\"Prior location + scale\"], label=\"Prior location + scale\")\n",
    "#           ]\n",
    "#plt.legend(handles=handles, loc=\"upper center\", ncol=4, bbox_to_anchor=(0, -1.3), title=\"\", fontsize=20)\n",
    "\n",
    "absmax = np.ceil(np.abs(df.iloc[:, 1:].values).max())\n",
    "ticks = np.arange(-100, 100, 5)\n",
    "ticks = ticks[(ticks>-absmax) & (ticks<absmax)]\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xlim(-absmax, absmax)\n",
    "    ax.set_ylim(-absmax, absmax)\n",
    "    \n",
    "    #ax.set_xticks(ticks)\n",
    "    #ax.set_yticks(ticks)\n",
    "    \n",
    "\n",
    "plt.savefig(f\"plots/abf_mvn_means_sufficient_pairplot.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(0.1,0.1))\n",
    "plt.gca().set_axis_off()\n",
    "handles = [mpatches.Patch(facecolor=color_codes[\"No MMS\"], label=r\"No MMS\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"Prior location\"], label=r\"Prior location: $\\mu_0=5$\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"Prior scale\"], label=r\"Prior scale: $\\tau_0=2.5$\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"Prior location + scale\"], label=r\"Prior location and scale: $\\mu_0=5, \\tau_0=2.5$\")\n",
    "           ]\n",
    "plt.legend(handles=handles, loc=\"center\", ncol=4, title=\"\" ,fontsize=20, labelspacing=2)\n",
    "plt.savefig(f\"plots/abf_mvn_means_sufficient_pairplot_MMD_legend.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplot overcomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = 10000\n",
    "\n",
    "# No MMS\n",
    "_, x_no_MMS = GenerativeModel(\n",
    "    GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1), \n",
    "    GaussianMeanSimulator(D=D))(n_sim, 100)\n",
    "\n",
    "_, x_simulator_MMS = GenerativeModel(\n",
    "    GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1), \n",
    "    GaussianMeanSimulator(D=D, s=10.0))(n_sim, 100)\n",
    "\n",
    "_, x_noise_MMS = GenerativeModel(\n",
    "    GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1), \n",
    "    GaussianMeanSimulator(D=D))(n_sim, 100)\n",
    "\n",
    "x_noise_MMS = noisify_x(x_noise_MMS, lamda = 0.5,\n",
    "                          noise_sampler = partial(beta_noise_sampler, a=2, b=5, tau=1)\n",
    "                         )\n",
    "\n",
    "s_no_MMS = np.array(trainer_overcomplete.network.summary_net(x_no_MMS))\n",
    "s_simulator_MMS = np.array(trainer_overcomplete.network.summary_net(x_simulator_MMS))\n",
    "s_noise_MMS = np.array(trainer_overcomplete.network.summary_net(x_noise_MMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAMES = ['Simulator', 'No MMS', 'Noise']\n",
    "S_ALL = [s_simulator_MMS, s_no_MMS, s_noise_MMS]\n",
    "DF = (pd.DataFrame(s, columns=[r'$s_{%i}$'%i for i in range(1, S_overcomplete+1)]) for s in S_ALL)\n",
    "\n",
    "df = pd.concat(DF,\n",
    "              keys=TASK_NAMES,\n",
    "              names=['MMS', None]\n",
    "              ).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset(x, y, hue, mask, **kws):\n",
    "    sns.kdeplot(x=x[mask], y=y[mask], hue=hue[mask], fill=True, levels=10, antialiased=True, **kws)\n",
    "\n",
    "g = sns.PairGrid(df, hue=\"MMS\", height=3, palette=[color_codes[task_name] for task_name in TASK_NAMES])\n",
    "\n",
    "g.map_diag(sns.kdeplot, linewidth=2)\n",
    "\n",
    "g.map_lower(plot_subset, mask= \n",
    "            (\n",
    "                (df[\"MMS\"] == 'Simulator') |\n",
    "                (df[\"MMS\"] == 'No MMS')\n",
    "           )\n",
    "           )\n",
    "\n",
    "\n",
    "g.map_upper(plot_subset, mask= \n",
    "            (\n",
    "                (df[\"MMS\"] == 'No MMS') |\n",
    "                (df[\"MMS\"] == 'Noise') \n",
    "                \n",
    "           )\n",
    "           )\n",
    "\n",
    "handles = [mpatches.Patch(facecolor=color_codes[\"No MMS\"], label=r\"No MMS\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"Simulator\"], label=r\"Simulator: $\\tau=10$\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"Noise\"], label=r\"Noise: $\\lambda=0.5$\")\n",
    "           ]\n",
    "\n",
    "plt.legend(handles=handles, loc=\"upper center\", ncol=4, bbox_to_anchor=(-1.4, -1.3), title=\"\", fontsize=20)\n",
    "\n",
    "absmax = 5.01 #np.ceil(np.abs(df.iloc[:, 1:].values).max())\n",
    "ticks = np.arange(-100, 100, 5)\n",
    "ticks = ticks[(ticks>-absmax) & (ticks<absmax)]\n",
    "\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xlim(-absmax, absmax)\n",
    "    ax.set_ylim(-absmax, absmax)\n",
    "    \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    \n",
    "plt.savefig(f\"plots/abf_mvn_means_overcomplete_pairplot.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)   ## bbox tight    # tiff oder pdf, dpi > 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(0.1,0.1))\n",
    "plt.gca().set_axis_off()\n",
    "handles = [mpatches.Patch(facecolor=color_codes[\"No MMS\"], label=r\"No MMS\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"Simulator\"], label=r\"Simulator scale: $\\tau=10$\"),\n",
    "           mpatches.Patch(facecolor=color_codes[\"Noise\"], label=r\"Noise fraction: $\\lambda=0.5$\")\n",
    "           ]\n",
    "plt.legend(handles=handles, loc=\"center\", ncol=4, title=\"\" ,fontsize=20, labelspacing=2)\n",
    "plt.savefig(f\"plots/abf_mvn_means_overcomplete_pairplot_MMD_legend.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrongness: MMD vs. Posterior Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_analytic_posterior(prior, simulator, x):\n",
    "    n_sim, n_obs, D = x.shape\n",
    "\n",
    "    # Set up variables\n",
    "    x_bar = np.mean(x, axis=1)                 # empirical mean\n",
    "    sigma_0 = np.eye(D) * prior.mu_scale       # mu prior covariance\n",
    "    sigma_0_inv = np.linalg.inv(sigma_0)       # inverse mu prior covariance\n",
    "    mu_0 = np.ones((D, 1)) * prior.mu_mean     # mu prior mean\n",
    "    sigma = simulator.sigma                    # likelihood covariance\n",
    "    sigma_inv = np.linalg.inv(sigma)           # inverse likelihood covariance\n",
    "\n",
    "    mu_posterior_covariance = np.stack([np.linalg.inv(sigma_0_inv + n_obs*sigma_inv)] * n_sim)\n",
    "\n",
    "    mu_posterior_mean = mu_posterior_covariance @ (sigma_0_inv @ mu_0 + n_obs * (sigma_inv @ x_bar[..., np.newaxis]))   \n",
    "    mu_posterior_mean = mu_posterior_mean.reshape(n_sim, D)\n",
    "\n",
    "    return mu_posterior_mean, mu_posterior_covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD_error_analysis_prior_MMS(trainer_sufficient, trainer_overcomplete, n_sim=200, n_steps=1000, n_posterior_samples=200):\n",
    "        \n",
    "    prior_star = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "    simulator_star = GaussianMeanSimulator(D=D)\n",
    "    generative_model_star = GenerativeModel(prior_star, simulator_star)\n",
    "    _, x_star = generative_model_star(n_sim, 100)\n",
    "    \n",
    "    s_star_sufficient = trainer_sufficient.network.summary_net(x_star)\n",
    "    s_star_overcomplete = trainer_overcomplete.network.summary_net(x_star)\n",
    "\n",
    "    tau_0_min, tau_0_max = 0.0, 2.5\n",
    "    mu_0_min, mu_0_max = 0.0, 5.0\n",
    "    b = np.linspace(0, 1, num = n_steps)\n",
    "\n",
    "    mu_0_list = mu_0_min + b * (mu_0_max - mu_0_min)\n",
    "    tau_0_list = tau_0_min + b * (tau_0_max - tau_0_min)\n",
    "\n",
    "    df_sufficient =  pd.DataFrame(\n",
    "        {'mu_0' : np.zeros(n_steps), \n",
    "         'tau_0': np.zeros(n_steps), \n",
    "         'MMD': np.zeros(n_steps), \n",
    "         'RMSE': np.zeros(n_steps)\n",
    "        })\n",
    "    \n",
    "    df_overcomplete =  pd.DataFrame(\n",
    "        {'mu_0' : np.zeros(n_steps), \n",
    "         'tau_0': np.zeros(n_steps), \n",
    "         'MMD': np.zeros(n_steps), \n",
    "         'RMSE': np.zeros(n_steps)\n",
    "        })\n",
    "\n",
    "\n",
    "    for i in tqdm(range(n_steps)):\n",
    "        mu_0 = mu_0_list[i]\n",
    "        tau_0 = tau_0_list[i]\n",
    "        prior = GaussianMeanPrior(D=D, mu_mean=mu_0, mu_scale=tau_0)\n",
    "        simulator = GaussianMeanSimulator(D=D)\n",
    "        generative_model = GenerativeModel(prior, simulator)\n",
    "        theta = prior(n_sim)\n",
    "        x = simulator(theta, 100)\n",
    "        \n",
    "        s_sufficient = trainer_sufficient.network.summary_net(x)\n",
    "        s_overcomplete = trainer_overcomplete.network.summary_net(x)\n",
    "\n",
    "        MMD_sufficient = maximum_mean_discrepancy(s_sufficient, s_star_sufficient, squared=False)\n",
    "        MMD_overcomplete = maximum_mean_discrepancy(s_overcomplete, s_star_overcomplete, squared=False)\n",
    "\n",
    "        theta_est_samples_sufficient = trainer_sufficient.network.sample(x, n_samples=n_posterior_samples)\n",
    "        theta_est_samples_overcomplete = trainer_overcomplete.network.sample(x, n_samples=n_posterior_samples)\n",
    "\n",
    "        posterior_mean, posterior_covariance = calculate_analytic_posterior(prior_star, simulator_star, x)\n",
    "        posterior_sd = np.sqrt(posterior_covariance.diagonal(axis1=1, axis2=2))\n",
    "\n",
    "        RMSE_sufficient = mean_squared_error(theta_est_samples_sufficient.mean(axis=0), posterior_mean, squared=False)\n",
    "        RMSE_overcomplete = mean_squared_error(theta_est_samples_overcomplete.mean(axis=0), posterior_mean, squared=False)\n",
    "        \n",
    "        df_sufficient.iloc[i, ] = [mu_0, tau_0, MMD_sufficient, RMSE_sufficient]\n",
    "        df_overcomplete.iloc[i, ] = [mu_0, tau_0, MMD_overcomplete, RMSE_overcomplete]\n",
    "        \n",
    "        \n",
    "    return df_sufficient, df_overcomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RERUN_MMD_ERROR_EXPERIMENT:\n",
    "    df_sufficient_prior, df_overcomplete_prior = MMD_error_analysis_prior_MMS(trainer_sufficient, trainer_overcomplete, n_sim=1000, n_steps=1000, n_posterior_samples=1000)\n",
    "\n",
    "    df_sufficient_prior.to_pickle(\"data/MMD_Error/MMD_Error_prior_sufficient.pkl\")\n",
    "    df_overcomplete_prior.to_pickle(\"data/MMD_Error/MMD_Error_prior_overcomplete.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def vectwo(a, b):\n",
    "    return r\"$(\\begin{smallmatrix}%.1f\\\\%.1f\\end{smallmatrix})$\"%(a, b)\n",
    "\n",
    "def plot_MMD_error_all_trainers_prior(dfs, names, colors, mode, ylab=r\"RMSE($\\mu_p\\,||\\,\\mu_{\\hat{\\theta}})$\", lag=5):   \n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    \n",
    "    if mode == \"x_MMD\":\n",
    "        for df, name, color in zip(dfs, names, colors):\n",
    "            MMD = np.array(df['MMD'])\n",
    "            RMSE = np.array(df['RMSE'])\n",
    "\n",
    "            idx = np.argsort(MMD)\n",
    "\n",
    "            MMD = MMD[idx]\n",
    "            RMSE = RMSE[idx]\n",
    "\n",
    "            ax.plot(MMD, RMSE, alpha=.30, color=color)\n",
    "            ax.plot(MMD[lag-1:], moving_average(RMSE, n=lag), linewidth=3, color=color, label=name, alpha=1.0) \n",
    "\n",
    "\n",
    "        plt.title(f\"MMS: {MMS_type}\")\n",
    "        ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "        ax.set_ylabel(ylab)\n",
    "        ax.set_xlim(0)\n",
    "        sns.despine()\n",
    "        ax.grid(alpha=.50)\n",
    "        ax.legend(fontsize=18)\n",
    "        \n",
    "    elif mode == \"x_MMS_param\":\n",
    "        \n",
    "        for df, name, color in zip(dfs, names, colors):\n",
    "            param_1 = np.array(df.iloc[:, 0])\n",
    "            param_2 = np.array(df.iloc[:, 1])\n",
    "            \n",
    "            MMD = np.array(df['MMD'])\n",
    "            RMSE = np.array(df['RMSE'])\n",
    "\n",
    "            idx = np.argsort(MMD)\n",
    "\n",
    "            MMD = MMD[idx]\n",
    "            RMSE = RMSE[idx]\n",
    "\n",
    "            scatterplot = ax.scatter(param_1, RMSE, c=MMD, alpha=.60, cmap='viridis', vmin=0, vmax=3)\n",
    "            ax.plot(param_1[lag-1:], moving_average(RMSE, n=lag), linewidth=3, color=\"black\", alpha=1.0) \n",
    "            ax.annotate(xy=(param_1[-1],moving_average(RMSE, n=lag)[-1]), xytext=(5,0), textcoords='offset points', fontsize=16, text=name, va='center')\n",
    "\n",
    "\n",
    "        #plt.title(f\"MMS: Prior [development note]\")\n",
    "        ax.set_xlabel(r\"Prior MMS $(\\begin{smallmatrix}\\mu_0 \\\\ \\tau_0 \\end{smallmatrix})$\")\n",
    "        #ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "        ax.set_ylabel(ylab)\n",
    "        ax.set_xlim(0)\n",
    "        ax.set_xticks([0,1,2,3,4,5])\n",
    "        ax.set_xticklabels([\n",
    "            r\"$(\\begin{smallmatrix}0\\\\ 1 \\end{smallmatrix})$\",\n",
    "            \"\", \"\", \"\", \"\", \n",
    "            r\"$(\\begin{smallmatrix}5\\\\ 2.5 \\end{smallmatrix})$\"\n",
    "        ])\n",
    "        sns.despine()\n",
    "        ax.grid(alpha=.50)\n",
    "        \n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=1.2,)\n",
    "\n",
    "        cbar = plt.colorbar(scatterplot, cax=cax)\n",
    "        cbar.solids.set(alpha=1)\n",
    "        cbar.ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "\n",
    "\n",
    "    plt.savefig(f\"plots/abf_mvn_means_sufficient_vs_overcomplete_MMD_Error_Prior.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/MMD_Error/MMD_Error_prior_sufficient.pkl\"):\n",
    "    df_sufficient_prior = pd.read_pickle(\"data/MMD_Error/MMD_Error_prior_sufficient.pkl\")\n",
    "if os.path.isfile(\"data/MMD_Error/MMD_Error_prior_overcomplete.pkl\"):\n",
    "    df_overcomplete_prior = pd.read_pickle(\"data/MMD_Error/MMD_Error_prior_overcomplete.pkl\")\n",
    "    \n",
    "plot_MMD_error_all_trainers_prior(\n",
    "                            [df_sufficient_prior, df_overcomplete_prior],\n",
    "                            ['minimal', 'overcomplete'],\n",
    "                            ['tab:blue', 'tab:red'],\n",
    "                            mode = \"x_MMS_param\",\n",
    "                            lag=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD_error_analysis_noise_likelihood_MMS(trainer_sufficient, trainer_overcomplete, n_sim=200, n_steps=1000, n_posterior_samples=200):\n",
    "        \n",
    "    prior_star = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "    simulator_star = GaussianMeanSimulator(D=D)\n",
    "    generative_model_star = GenerativeModel(prior_star, simulator_star)\n",
    "    _, x_star = generative_model_star(n_sim, 100)\n",
    "    \n",
    "    s_star_sufficient = trainer_sufficient.network.summary_net(x_star)\n",
    "    s_star_overcomplete = trainer_overcomplete.network.summary_net(x_star)\n",
    "\n",
    "    tau_min, tau_max = 1.0, 20.0\n",
    "    lamda_min, lamda_max = 0.0, 1.0\n",
    "    b = np.linspace(0, 1, num = n_steps)\n",
    "\n",
    "    lamda_list = lamda_min + b * (lamda_max - lamda_min)\n",
    "    tau_list = tau_min + b * (tau_max - tau_min)\n",
    "\n",
    "    df_sufficient =  pd.DataFrame(\n",
    "        {'lamda' : np.zeros(n_steps), \n",
    "         'tau': np.zeros(n_steps), \n",
    "         'MMD': np.zeros(n_steps), \n",
    "         'RMSE': np.zeros(n_steps)\n",
    "        })\n",
    "    \n",
    "    df_overcomplete =  pd.DataFrame(\n",
    "        {'lamda' : np.zeros(n_steps), \n",
    "         'tau': np.zeros(n_steps), \n",
    "         'MMD': np.zeros(n_steps), \n",
    "         'RMSE': np.zeros(n_steps)\n",
    "        })\n",
    "\n",
    "\n",
    "    for i in tqdm(range(n_steps)):\n",
    "        lamda = lamda_list[i]\n",
    "        tau = tau_list[i]\n",
    "        prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "        simulator = GaussianMeanSimulator(D=D, s = tau)\n",
    "        theta = prior(n_sim)\n",
    "        x = simulator(theta, 100)\n",
    "        \n",
    "        x = noisify_x(x, lamda = lamda,\n",
    "                      noise_sampler = partial(beta_noise_sampler, a=2, b=5, tau=tau))\n",
    "        \n",
    "        s_sufficient = trainer_sufficient.network.summary_net(x)\n",
    "        s_overcomplete = trainer_overcomplete.network.summary_net(x)\n",
    "\n",
    "        MMD_sufficient = maximum_mean_discrepancy(s_sufficient, s_star_sufficient, squared=False)\n",
    "        MMD_overcomplete = maximum_mean_discrepancy(s_overcomplete, s_star_overcomplete, squared=False)\n",
    "\n",
    "        theta_est_samples_sufficient = trainer_sufficient.network.sample(x, n_samples=n_posterior_samples)\n",
    "        theta_est_samples_overcomplete = trainer_overcomplete.network.sample(x, n_samples=n_posterior_samples)\n",
    "\n",
    "        posterior_mean, posterior_covariance = calculate_analytic_posterior(prior_star, simulator_star, x)\n",
    "        #posterior_sd = np.sqrt(posterior_covariance.diagonal(axis1=1, axis2=2))\n",
    "\n",
    "        RMSE_sufficient = mean_squared_error(theta_est_samples_sufficient.mean(axis=0), posterior_mean, squared=False)\n",
    "        RMSE_overcomplete = mean_squared_error(theta_est_samples_overcomplete.mean(axis=0), posterior_mean, squared=False)\n",
    "        \n",
    "        df_sufficient.iloc[i, ] = [lamda, tau, MMD_sufficient, RMSE_sufficient]\n",
    "        df_overcomplete.iloc[i, ] = [lamda, tau, MMD_overcomplete, RMSE_overcomplete]\n",
    "        \n",
    "        \n",
    "    return df_sufficient, df_overcomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RERUN_MMD_ERROR_EXPERIMENT:\n",
    "    df_sufficient_noise_likelihood, df_overcomplete_noise_likelihood = MMD_error_analysis_noise_likelihood_MMS(trainer_sufficient, trainer_overcomplete, n_sim=1000, n_steps=1000, n_posterior_samples=1000)\n",
    "\n",
    "\n",
    "    df_sufficient_noise_likelihood.to_pickle(\"data/MMD_Error/MMD_Error_noise_likelihood_sufficient.pkl\")\n",
    "    df_overcomplete_noise_likelihood.to_pickle(\"data/MMD_Error/MMD_Error_noise_likelihood_overcomplete.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot_MMD_error_all_trainers_noise_likelihood(dfs, names, colors, mode, ylab=r\"RMSE($\\mu_p\\,||\\,\\mu_{\\hat{\\theta}})$\", lag=5):   \n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    \n",
    "    if mode == \"x_MMD\":\n",
    "        for df, name, color in zip(dfs, names, colors):\n",
    "            MMD = np.array(df['MMD'])\n",
    "            RMSE = np.array(df['RMSE'])\n",
    "\n",
    "            idx = np.argsort(MMD)\n",
    "\n",
    "            MMD = MMD[idx]\n",
    "            RMSE = RMSE[idx]\n",
    "\n",
    "            ax.plot(MMD, RMSE, alpha=.30, color=color)\n",
    "            ax.plot(MMD[lag-1:], moving_average(RMSE, n=lag), linewidth=3, color=color, label=name, alpha=1.0) \n",
    "\n",
    "\n",
    "        plt.title(f\"MMS: {MMS_type}\")\n",
    "        ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "        ax.set_ylabel(ylab)\n",
    "        ax.set_xlim(0)\n",
    "        sns.despine()\n",
    "        ax.grid(alpha=.50)\n",
    "        ax.legend(fontsize=18)\n",
    "        \n",
    "    elif mode == \"x_MMS_param\":\n",
    "        \n",
    "        for df, name, color in zip(dfs, names, colors):\n",
    "            #outlier_idx = np.where(np.array(df['RMSE']) > np.quantile(np.array(df['RMSE']), .95))[0]\n",
    "            #df = df.drop(outlier_idx)\n",
    "            \n",
    "            param_1 = np.array(df.iloc[:, 0])\n",
    "            param_2 = np.array(df.iloc[:, 1])\n",
    "            \n",
    "            MMD = np.array(df['MMD'])\n",
    "            RMSE = np.array(df['RMSE'])\n",
    "\n",
    "            idx = np.argsort(MMD)\n",
    "\n",
    "            MMD = MMD[idx]\n",
    "            RMSE = RMSE[idx]\n",
    "\n",
    "            scatterplot = ax.scatter(param_1, RMSE, c=MMD, alpha=.40, cmap='viridis', vmin=0, vmax=3)\n",
    "            ax.plot(param_1[lag-1:], moving_average(RMSE, n=lag), linewidth=3, color=\"black\", alpha=1.0) \n",
    "            ax.annotate(xy=(param_1[-1],moving_average(RMSE, n=lag)[-1]), xytext=(5,0), textcoords='offset points', fontsize=16, text=name, va='center')\n",
    "\n",
    "\n",
    "        #plt.title(f\"MMS: Noise/Likelihood [development note]\")\n",
    "        ax.set_xlabel(r\"Noise and Simulator MMS $(\\begin{smallmatrix}\\lambda \\\\ \\tau \\end{smallmatrix})$\")\n",
    "        #ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "        ax.set_ylabel(ylab)\n",
    "        ax.set_xlim(0)\n",
    "        ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        ax.set_xticklabels([\n",
    "            r\"$(\\begin{smallmatrix}0\\\\ 1 \\end{smallmatrix})$\",\n",
    "            \"\", \"\", \"\", \"\", \n",
    "            r\"$(\\begin{smallmatrix}1\\\\ 20 \\end{smallmatrix})$\"\n",
    "        ])\n",
    "        sns.despine()\n",
    "        ax.grid(alpha=.50)\n",
    "        \n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=1.2)\n",
    "\n",
    "        cbar = plt.colorbar(scatterplot, cax=cax)\n",
    "        cbar.solids.set(alpha=1)\n",
    "        cbar.ax.set_xlabel(r\"$\\widehat{\\mathrm{rMMD}}$\")\n",
    "\n",
    "\n",
    "    plt.savefig(f\"plots/abf_mvn_means_sufficient_vs_overcomplete_MMD_Error_Noise_Likelihood.{FILEFORMAT}\", bbox_inches=\"tight\", dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"data/MMD_Error/MMD_Error_noise_likelihood_sufficient.pkl\"):\n",
    "    df_sufficient_noise_likelihood = pd.read_pickle(\"data/MMD_Error/MMD_Error_noise_likelihood_sufficient.pkl\")\n",
    "if os.path.isfile(\"data/MMD_Error/MMD_Error_noise_likelihood_overcomplete.pkl\"):\n",
    "    df_overcomplete_noise_likelihood = pd.read_pickle(\"data/MMD_Error/MMD_Error_noise_likelihood_overcomplete.pkl\")\n",
    "    \n",
    "\n",
    "plot_MMD_error_all_trainers_noise_likelihood( \n",
    "                            [df_sufficient_noise_likelihood, df_overcomplete_noise_likelihood],\n",
    "                            ['minimal', 'overcomplete'],\n",
    "                            ['tab:blue', 'tab:red'],\n",
    "                            mode=\"x_MMS_param\",\n",
    "                            lag=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Noise visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    tau_min, tau_max = 1.0, 10.0\n",
    "    lamda_min, lamda_max = 0.0, 1.0\n",
    "\n",
    "    n_tau = 10\n",
    "    n_lamda = 10\n",
    "\n",
    "    tau_list = np.linspace(tau_min, tau_max, num=n_tau)\n",
    "    lamda_list = np.linspace(lamda_min, lamda_max, num=n_lamda)\n",
    "\n",
    "    fig, axes = plt.subplots(n_tau, n_lamda, figsize=(25, 25))\n",
    "    nrows, ncols = axes.shape\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            tau = tau_list[row]\n",
    "            lamda = lamda_list[col]\n",
    "            ax = axes[row, col]\n",
    "            ax.set_ylabel(\"\")\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlim(-20, 20)\n",
    "            ax.set_title(r\"$\\lambda=%.1f, \\tau=%.1f$\"%(lamda, tau))\n",
    "            for _ in range(3):\n",
    "                theta, x = trainer_sufficient._forward_inference(1, 500)\n",
    "                prior = GaussianMeanPrior(D=D, mu_mean=0, mu_scale=1)\n",
    "                simulator = GaussianMeanSimulator(D=D, s = tau)\n",
    "                theta = prior(n_sim)\n",
    "                x = simulator(theta, 100)\n",
    "                x = noisify_x(x, lamda = lamda,\n",
    "                              noise_sampler = partial(beta_noise_sampler, a=2, b=5, tau=tau)\n",
    "                             )\n",
    "                x = x[0, :, 0]\n",
    "                sns.kdeplot(x, ax=ax, alpha=.40)\n",
    "\n",
    "    sns.despine()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"plots/abf_mvn_data_noise_likelihood.jpg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
